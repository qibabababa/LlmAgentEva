#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„æµ‹å¼•æ“
æ ¸å¿ƒè¯„æµ‹é€»è¾‘
"""

import sys
import json
import time
import traceback
from pathlib import Path
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from lib.core.config_manager import get_config
from lib.core.logger import get_logger
from lib.api.client import APIClient, APIError
from lib.api.judge_client import get_judge_client
from lib.tools.tool_executor import run_tool_calls
from lib.core.utils import read_json, append_to_json_file

# å¯¼å…¥éªŒè¯å™¨
from lib.validators.bugcode import validate
from lib.validators.convert import validate_js_cases
from lib.validators.refactor import validate_refactor
from lib.validators.env import validate_env
from lib.validators.summary import validate_sum
from lib.validators.split import validate_split
from lib.core.simple_data_manager import get_simple_data_manager

# åˆ›å»ºlogger
logger = get_logger(__name__)


class EvaluationEngine:
    """è¯„æµ‹å¼•æ“"""
    
    def __init__(self, model: Optional[str] = None, use_stream: bool = True):
        """
        åˆå§‹åŒ–è¯„æµ‹å¼•æ“
        
        Args:
            model: æ¨¡å‹åç§°ï¼ˆè¢«æµ‹è¯•çš„æ¨¡å‹ï¼‰
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼API
        """
        self.config = get_config()
        self.model = model or self.config.api.default_model
        self.use_stream = use_stream and self.config.api.stream_enabled
        self.client = APIClient(model=self.model)
        
        # åˆå§‹åŒ–Judgeå®¢æˆ·ç«¯ï¼ˆç”¨äºè¯„ä¼°sum/splitä»»åŠ¡ï¼‰
        self.judge_client = get_judge_client()
        
        logger.info(f"è¯„æµ‹å¼•æ“å·²åˆå§‹åŒ–: model={self.model}, stream={self.use_stream}")
        if self.judge_client.available:
            judge_model = self.config.get('evaluation', {}).get('judge_model', {}).get('model', 'Unknown')
            logger.info(f"Judgeå®¢æˆ·ç«¯å¯ç”¨: model={judge_model}")
        else:
            logger.info("Judgeå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œsum/splitä»»åŠ¡å°†ä½¿ç”¨è§„åˆ™è¯„ä¼°")
        
        logger.info(f"è¯„æµ‹å¼•æ“å·²åˆå§‹åŒ–: model={self.model}, stream={self.use_stream}")
        
    def run_single_task(self, 
                       question: Dict[str, Any],
                       ground_truth: Dict[str, Any],
                       system_prompt: str,
                       tools: List[Dict],
                       output_file: Path) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªè¯„æµ‹ä»»åŠ¡
        
        Args:
            question: é—®é¢˜æ•°æ®
            ground_truth: æ ‡å‡†ç­”æ¡ˆ
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·å®šä¹‰
            output_file: è¾“å‡ºæ–‡ä»¶
            
        Returns:
            è¯„æµ‹ç»“æœ
        """
        answer = dict(question)
        answer["use_tools"] = []
        answer["metrics"] = {
            "total_rounds": 0,
            "tool_calls": 0,
            "tool_types": {},  # æ¯ç§å·¥å…·çš„è°ƒç”¨æ¬¡æ•°
            "output_chars": 0
        }
        current_step = "start"
        tmp_files: List[Path] = []
        
        try:
            # æ„é€ åˆå§‹æ¶ˆæ¯
            current_step = "prepare_messages"
            logger.debug(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: tag={question.get('tag')}, number={question.get('number')}")
            
            base_path = str(self.config.tasks.data_dirs.get(question['tag'], Path.cwd()))
            logger.debug(f"å·¥ä½œç›®å½•: {base_path}")
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "##ä»»åŠ¡åˆ†è§£:\n" + ground_truth['question']},
                {"role": "assistant", "content": question['answer']},
                {"role": "user", "content": "æˆ‘äº†è§£ä½ çš„ç­”æ¡ˆäº†ï¼Œç»“åˆä½ çš„ç­”æ¡ˆå’Œæˆ‘ä¸ªäººçš„ç†è§£ï¼Œæˆ‘å¾—åˆ°äº†ä¸€ä¸ªåˆ†è§£åçš„ä»»åŠ¡åˆ—è¡¨ï¼Œç°åœ¨è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼š\n" + str(ground_truth["SubTasks"])},
                {"role": "assistant", "content": str(question["plan_answer"])},
                {"role": "user", "content": "å½“å‰å·¥ä½œç›®å½•ï¼š" + base_path + " \n\n ##ä»»åŠ¡æ‰§è¡Œ:\n" + str(ground_truth["plan_answer"]) + question.get("sums", "")},
            ]
            
            # å¤šè½®å¯¹è¯
            round_idx = 0
            max_rounds = self.config.tasks.max_rounds
            logger.info(f"å¼€å§‹å¤šè½®å¯¹è¯ï¼Œæœ€å¤§è½®æ•°: {max_rounds}")
            
            while round_idx < max_rounds:
                round_idx += 1
                logger.info(f"===== {self.model} Round {round_idx}/{max_rounds} =====")
                current_step = f"api_round_{round_idx}"
                
                try:
                    # è°ƒç”¨API
                    if self.use_stream:
                        logger.info("æ­£åœ¨ä»¥æµå¼æ¨¡å¼è°ƒç”¨API...")
                        try:
                            stream = self.client.chat_completion_stream(
                                messages=messages,
                                tools=tools
                            )
                            resp = self.client.reconstruct_from_stream(stream)
                            logger.debug("æµå¼APIè°ƒç”¨æˆåŠŸ")
                        except (APIError, Exception) as stream_error:
                            logger.error(f"æµå¼APIå¤±è´¥: {stream_error}")
                            if self.config.api.stream_fallback:
                                logger.info("å°è¯•fallbackåˆ°éæµå¼API")
                                resp = self.client.chat_completion(
                                    messages=messages,
                                    tools=tools
                                )
                                logger.info("éæµå¼API fallbackæˆåŠŸ")
                            else:
                                raise
                    else:
                        logger.info("æ­£åœ¨ä»¥éæµå¼æ¨¡å¼è°ƒç”¨API...")
                        resp = self.client.chat_completion(
                            messages=messages,
                            tools=tools
                        )
                    
                    # éªŒè¯å“åº”
                    if not resp or 'choices' not in resp:
                        error_msg = f"APIè¿”å›äº†æ— æ•ˆçš„å“åº”: {resp}"
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                    
                    # ä¿å­˜å“åº”
                    answer[f"round{round_idx}"] = resp['choices'][0]['message']
                    messages.append(answer[f"round{round_idx}"])
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if not answer[f'round{round_idx}'].get('tool_calls'):
                        logger.warning("æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡ç»“æŸ")
                        break
                    
                    # æ‰§è¡Œå·¥å…·
                    current_step = f"run_tool_round_{round_idx}"
                    tool_calls = list(answer[f'round{round_idx}']['tool_calls'])
                    tool_name = tool_calls[0]['function']['name']
                    logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}")
                    
                    # æ ¹æ®ä»»åŠ¡ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
                    use_venv = (question['tag'] == "env")
                    venv_path = None
                    if use_venv:
                        logger.debug("ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒæ‰§è¡Œ")
                        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è™šæ‹Ÿç¯å¢ƒè·¯å¾„
                        venv_path = self.config.paths.venv_dir
                    
                    try:
                        model_run_out = run_tool_calls(tool_calls, base_path, use_venv, venv_path)
                        logger.debug(f"å·¥å…·æ‰§è¡Œå®Œæˆ: {tool_name}")
                    except Exception as tool_error:
                        logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_error}")
                        raise
                    
                    # è®°å½•ä½¿ç”¨çš„å·¥å…·
                    answer['use_tools'].append(tool_calls[0]['function']['name'])
                    
                    # æ›´æ–°æŒ‡æ ‡
                    tool_name_key = tool_calls[0]['function']['name']
                    answer['metrics']['tool_calls'] += 1
                    if tool_name_key not in answer['metrics']['tool_types']:
                        answer['metrics']['tool_types'][tool_name_key] = 0
                    answer['metrics']['tool_types'][tool_name_key] += 1
                    
                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                    # æ£€æŸ¥å·¥å…·æ‰§è¡Œæ˜¯å¦å‡ºé”™
                    if "error" in model_run_out:
                        # å·¥å…·æ‰§è¡Œå¤±è´¥
                        tool_result_content = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {model_run_out['error']}"
                        logger.warning(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {model_run_out['error']}")
                    else:
                        # å·¥å…·æ‰§è¡ŒæˆåŠŸ
                        tool_result_content = model_run_out.get("result", "")
                    
                    answer[f"round{round_idx}_tool_call"] = {
                        "role": "tool",
                        "tool_call_id": model_run_out.get("id", ""),
                        "content": tool_result_content
                    }
                    messages.append({
                        "role": "tool",
                        "tool_call_id": model_run_out.get("id", ""),
                        "content": tool_result_content
                    })
                    
                except APIError as api_error:
                    logger.error(f"APIé”™è¯¯ (Round {round_idx}): {api_error}")
                    answer[f'round{round_idx}_error'] = {
                        'type': 'APIError',
                        'message': str(api_error),
                        'status_code': getattr(api_error, 'status_code', None)
                    }
                    break
                except Exception as api_error:
                    logger.error(f"Round {round_idx} å¤±è´¥: {api_error}\n{traceback.format_exc()}")
                    answer[f'round{round_idx}_error'] = {
                        'type': type(api_error).__name__,
                        'message': str(api_error)
                    }
                    break
            
            # éªŒè¯ç»“æœ
            current_step = "validate_result"
            logger.info("å¼€å§‹éªŒè¯ç»“æœ")
            answer['pass'] = self._validate_result(question, tmp_files)
            result_text = "é€šè¿‡" if answer['pass'] else "å¤±è´¥"
            logger.info(f"éªŒè¯ç»“æœ: {result_text}")
            
            # ç»Ÿè®¡æŒ‡æ ‡
            # ç»Ÿè®¡å¯¹è¯è½®æ¬¡
            round_count = 0
            for key in answer.keys():
                if key.startswith('round') and not key.endswith('_tool_call') and not key.endswith('_error'):
                    round_count += 1
            answer['metrics']['total_rounds'] = round_count
            
            # ç»Ÿè®¡è¾“å‡ºå­—ç¬¦æ•°
            total_output = 0
            for key, value in answer.items():
                if key.startswith('round') and isinstance(value, dict):
                    content = value.get('content', '')
                    if content:
                        total_output += len(str(content))
            answer['metrics']['output_chars'] = total_output
            
            # è®°å½•å·¥å…·è°ƒç”¨ç§ç±»æ•°
            answer['metrics']['unique_tools'] = len(answer['metrics']['tool_types'])
            
        except FileNotFoundError as exc:
            logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ° '{current_step}': {exc}")
            answer['pass'] = False
            answer['fail_step'] = current_step
            answer['error'] = f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(exc)}"
            answer['error_type'] = 'FileNotFoundError'
            logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ° '{current_step}': {exc}")
        except json.JSONDecodeError as exc:
            logger.error(f"JSONè§£æé”™è¯¯ '{current_step}': {exc}")
            answer['pass'] = False
            answer['fail_step'] = current_step
            answer['error'] = f"JSONè§£æé”™è¯¯: {str(exc)}"
            answer['error_type'] = 'JSONDecodeError'
            logger.error(f"JSONè§£æé”™è¯¯ '{current_step}': {exc}")
        except APIError as exc:
            logger.error(f"APIé”™è¯¯ '{current_step}': {exc}")
            answer['pass'] = False
            answer['fail_step'] = current_step
            answer['error'] = str(exc)
            answer['error_type'] = 'APIError'
            answer['status_code'] = getattr(exc, 'status_code', None)
            logger.error(f"APIé”™è¯¯ '{current_step}': {exc}")
        except Exception as exc:
            logger.error(f"æœªé¢„æœŸé”™è¯¯ '{current_step}': {exc}\n{traceback.format_exc()}")
            answer['pass'] = False
            answer['fail_step'] = current_step
            answer['error'] = str(exc)
            answer['error_type'] = type(exc).__name__
            answer['traceback'] = traceback.format_exc()
            logger.error(f"é”™è¯¯ '{current_step}': {exc}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if tmp_files:
                logger.debug(f"æ¸…ç† {len(tmp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")
            self._cleanup_temp_files(tmp_files)
        
        # ä¿å­˜ç»“æœ
        answer['messages'] = messages
        append_to_json_file(answer, output_file)
        
        return answer
    
    def _validate_result(self, question: Dict[str, Any], tmp_files: List[Path]) -> bool:
        """éªŒè¯ç»“æœ"""
        num = question['number']
        tag = question['tag']
        
        logger.debug(f"éªŒè¯ä»»åŠ¡: tag={tag}, number={num}")
        
        try:
            if tag == "fix_bug":
                fixed_file = self.config.tasks.data_dirs['fix_bug'] / f"fix_code_{num}.py"
                test_file = self.config.tasks.data_dirs['bug_test'].parent / "bug_test" / f"test_{num}.txt"
                tmp_files.append(fixed_file)
                logger.debug(f"éªŒè¯bugä¿®å¤: fixed={fixed_file}, test={test_file}")
                result = validate(fixed_file, test_file, list(question["test_case"]))
                return result
            
            elif tag == "convert":
                js_file = self.config.tasks.data_dirs['convert'] / f"js_{num}.js"
                cases_file = self.config.tasks.data_dirs['convert'] / f"case_{num}.json"
                tmp_files.append(js_file)
                logger.debug(f"éªŒè¯ä»£ç è½¬æ¢: js={js_file}")
                result = validate_js_cases(js_file, cases_file)
                return result
            
            elif tag == "refactor":
                # Refactorä»»åŠ¡ï¼šæ¨¡å‹ä¼šç›´æ¥ä¿®æ”¹æ–‡ä»¶
                refactor_file = self.config.tasks.data_dirs['refactor'] / f"utils_{num}.py"
                expected_output_file = self.config.tasks.data_dirs['refactor'] / f"expected_output_{num}.txt"
                
                logger.debug(f"éªŒè¯ä»£ç é‡æ„: file={refactor_file}")
                logger.debug(f"  é‡å‘½åæ˜ å°„: {question.get('names', {})}")
                
                # è°ƒç”¨æ–°çš„éªŒè¯å™¨
                result = validate_refactor(
                    file_path=str(refactor_file),
                    rename_map=question.get("names", {}),
                    expected_output_file=str(expected_output_file) if expected_output_file.exists() else None,
                    run_script=True
                )
                return result
            
            elif tag == "env":
                env_file = self.config.tasks.data_dirs['env'] / f"env_{num}.py"
                logger.debug(f"éªŒè¯ç¯å¢ƒé…ç½®: file={env_file}")
                result = validate_env(env_file, venv_dir=str(self.config.paths.venv_dir))
                return result
            
            elif tag == "sum":
                md_file = self.config.tasks.data_dirs['sum'] / f"sample_scraper_{num}" / "README.md"
                src_dir = self.config.tasks.data_dirs['sum'] / f"sample_scraper_{num}" / "src"
                tmp_files.append(md_file)
                logger.debug(f"éªŒè¯æ€»ç»“: file={md_file}, src={src_dir}")
                
                # è°ƒç”¨validate_sumï¼Œä½¿ç”¨JudgeClientè€Œä¸æ˜¯ä¸»API client
                passed, details = validate_sum(
                    md_file=md_file,
                    src_dir=src_dir if src_dir.exists() else None,
                    judge_client=self.judge_client,  # ä½¿ç”¨Judgeå®¢æˆ·ç«¯
                    use_llm=True  # ä¼˜å…ˆä½¿ç”¨LLMè¯„ä¼°
                )
                logger.info(f"æ€»ç»“è¯„ä¼°: é€šè¿‡={passed}, è¯„åˆ†={details.get('score', 0):.2f}, æ–¹æ³•={details.get('method')}")
                return passed
            
            elif tag == "split":
                cases_file = self.config.tasks.data_dirs['split'] / f"case_{num}.py"
                fixed_file = self.config.tasks.data_dirs['split'] / f"fix_{num}.py"
                tmp_files.append(fixed_file)
                logger.debug(f"éªŒè¯ä»£ç æ‹†åˆ†: orig={cases_file}, split={fixed_file}")
                
                # è°ƒç”¨validate_splitï¼Œä½¿ç”¨JudgeClientè€Œä¸æ˜¯ä¸»API client
                passed, details = validate_split(
                    file_orig=str(cases_file),
                    file_split=str(fixed_file),
                    function_name=question.get("function", "giant_cleaner"),  # é»˜è®¤å‡½æ•°å
                    judge_client=self.judge_client,  # ä½¿ç”¨Judgeå®¢æˆ·ç«¯
                    use_llm=True,  # ä¼˜å…ˆä½¿ç”¨LLMè¯„ä¼°
                    mute=True
                )
                logger.info(f"æ‹†åˆ†è¯„ä¼°: é€šè¿‡={passed}, è¯„åˆ†={details.get('score', 0):.2f}, æ–¹æ³•={details.get('method')}")
                return passed
            
            else:
                logger.warning(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {tag}")
                logger.warning(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {tag}")
                return False
                
        except Exception as e:
            logger.error(f"éªŒè¯å¤±è´¥: {e}\n{traceback.format_exc()}")
            logger.error(f"éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _cleanup_temp_files(self, tmp_files: List[Path]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        for f in tmp_files:
            try:
                if f.exists():
                    f.unlink()
                    logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {f.name}")
            except Exception as e:
                logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {f}: {e}")
    
    def run_evaluation(self,
                      task_type: str = "all",
                      output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„æµ‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¯„æµ‹ç»Ÿè®¡ç»“æœ
        """
        # ä½¿ç”¨å¼ºåˆ¶æ•°æ®æ¢å¤
        data_manager = get_simple_data_manager()
        
        with data_manager.auto_restore_tasks():
            return self._run_evaluation_internal(task_type, output_dir)
    
    def _run_evaluation_internal(self,
                                 task_type: str = "all",
                                 output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        å†…éƒ¨è¯„æµ‹æ‰§è¡Œï¼ˆåœ¨æ•°æ®æ¢å¤ä¿æŠ¤ä¸‹è¿è¡Œï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¯„æµ‹ç»Ÿè®¡ç»“æœ
        """
        logger.info("="*70)
        logger.info("å¼€å§‹è¯„æµ‹")
        logger.info("="*70)
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.config.paths.outputs_dir / f"eval_{int(time.time())}"
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_file = self.config.paths.test_cases_dir / "exe_task_total.json"
        questions = read_json(test_file)
        
        # ç­›é€‰ä»»åŠ¡
        if task_type != "all":
            questions = [q for q in questions if q.get('tag') == task_type]
        
        logger.info(f"ä»»åŠ¡æ•°é‡: {len(questions)}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # åŠ è½½ç³»ç»Ÿæç¤ºè¯å’Œå·¥å…·
        system_prompt_file = self.config.paths.prompts_dir / "system_prompt_2.json"
        tool_list_file = self.config.paths.prompts_dir / "tool_list.json"
        
        system_prompt_data = read_json(system_prompt_file)
        tools = read_json(tool_list_file)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_system_prompt(system_prompt_data, questions)
        
        # è¿è¡Œè¯„æµ‹
        results = []
        for idx, question in enumerate(questions, 1):
            logger.info(f"{'='*70}")
            logger.info(f"ä»»åŠ¡ {idx}/{len(questions)}: {question.get('tag')} - {question.get('number')}")
            logger.info(f"{'='*70}")
            
            output_file = output_dir / f"result_{idx}.json"
            
            try:
                result = self.run_single_task(
                    question=question,
                    ground_truth=question,  # ä½¿ç”¨è‡ªèº«ä½œä¸ºGT
                    system_prompt=system_prompt,
                    tools=tools,
                    output_file=output_file
                )
                results.append(result)
                
                status = "âœ… é€šè¿‡" if result.get('pass') else "âŒ å¤±è´¥"
                logger.info(f"ç»“æœ: {status}")
                
                # è¾“å‡ºè¯¥ä»»åŠ¡çš„æŒ‡æ ‡
                metrics = result.get('metrics', {})
                logger.info(f"æŒ‡æ ‡: è½®æ¬¡={metrics.get('total_rounds', 0)}, "
                          f"å·¥å…·è°ƒç”¨={metrics.get('tool_calls', 0)}, "
                          f"å·¥å…·ç§ç±»={metrics.get('unique_tools', 0)}, "
                          f"è¾“å‡ºå­—ç¬¦={metrics.get('output_chars', 0)}")
                if metrics.get('tool_types'):
                    tool_list = ', '.join([f"{t}Ã—{c}" for t, c in metrics['tool_types'].items()])
                    logger.info(f"å·¥å…·è¯¦æƒ…: {tool_list}")
                
            except Exception as e:
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                logger.error(traceback.format_exc())
        
        # ç»Ÿè®¡ç»“æœ
        stats = self._calculate_stats(results)
        
        # ä¿å­˜ç»Ÿè®¡
        stats_file = output_dir / "summary.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        logger.info("="*70)
        logger.info("ğŸ“Š è¯„æµ‹å®Œæˆ - è¯¦ç»†ç»Ÿè®¡")
        logger.info("="*70)
        
        # åŸºæœ¬ç»Ÿè®¡
        logger.info(f"âœ… åŸºæœ¬ç»Ÿè®¡:")
        logger.info(f"  æ€»ä»»åŠ¡æ•°: {stats['total']}")
        logger.info(f"  é€šè¿‡: {stats['passed']} ({stats['pass_rate']:.1%})")
        logger.info(f"  å¤±è´¥: {stats['failed']}")
        
        # å·¥å…·è°ƒç”¨ç»Ÿè®¡
        tool_stats = stats['tool_stats']
        logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ç»Ÿè®¡:")
        logger.info(f"  æ€»è°ƒç”¨æ¬¡æ•°: {tool_stats['total_calls']}")
        logger.info(f"  å¹³å‡æ¯ä»»åŠ¡: {tool_stats['avg_calls_per_task']:.2f} æ¬¡")
        logger.info(f"  ä½¿ç”¨çš„å·¥å…·ç§ç±»: {len(tool_stats['tool_types'])} ç§")
        if tool_stats['tool_types']:
            logger.info(f"  å·¥å…·ä½¿ç”¨æ’è¡Œ:")
            sorted_tools = sorted(tool_stats['tool_types'].items(), 
                                key=lambda x: x[1], reverse=True)
            for tool_name, count in sorted_tools[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"    - {tool_name}: {count} æ¬¡")
        
        # å¯¹è¯è½®æ¬¡ç»Ÿè®¡
        round_stats = stats['round_stats']
        logger.info(f"ğŸ’¬ å¯¹è¯è½®æ¬¡ç»Ÿè®¡:")
        logger.info(f"  æ€»è½®æ¬¡: {round_stats['total_rounds']}")
        logger.info(f"  å¹³å‡è½®æ¬¡: {round_stats['avg_rounds']:.2f}")
        logger.info(f"  æœ€å¤§è½®æ¬¡: {round_stats['max_rounds']}")
        logger.info(f"  æœ€å°è½®æ¬¡: {round_stats['min_rounds']}")
        
        # è¾“å‡ºç»Ÿè®¡
        output_stats = stats['output_stats']
        logger.info(f"ğŸ“ è¾“å‡ºç»Ÿè®¡:")
        logger.info(f"  æ€»å­—ç¬¦æ•°: {output_stats['total_chars']:,}")
        logger.info(f"  å¹³å‡æ¯ä»»åŠ¡: {output_stats['avg_chars_per_task']:.0f} å­—ç¬¦")
        logger.info(f"  æœ€å¤§è¾“å‡º: {output_stats['max_chars']:,} å­—ç¬¦")
        logger.info(f"  æœ€å°è¾“å‡º: {output_stats['min_chars']:,} å­—ç¬¦")
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        logger.info(f"ğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
        for task_type, type_stats in stats['by_task_type'].items():
            logger.info(f"  {task_type}:")
            logger.info(f"    ä»»åŠ¡æ•°: {type_stats['total']}")
            logger.info(f"    é€šè¿‡ç‡: {type_stats['pass_rate']:.1%}")
            logger.info(f"    å¹³å‡è½®æ¬¡: {type_stats['avg_rounds']:.2f}")
            logger.info(f"    å·¥å…·è°ƒç”¨: {type_stats['tool_calls']} æ¬¡")
            logger.info(f"    å¹³å‡è¾“å‡º: {type_stats.get('avg_output_chars', 0):.0f} å­—ç¬¦")
        
        # é”™è¯¯ç»Ÿè®¡
        if stats['error_stats']['total_errors'] > 0:
            error_stats = stats['error_stats']
            logger.info(f"âŒ é”™è¯¯ç»Ÿè®¡:")
            logger.info(f"  æ€»é”™è¯¯æ•°: {error_stats['total_errors']}")
            if error_stats['error_types']:
                logger.info(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
                for error_type, count in error_stats['error_types'].items():
                    logger.info(f"    - {error_type}: {count} æ¬¡")
            if error_stats['fail_steps']:
                logger.info(f"  å¤±è´¥æ­¥éª¤åˆ†å¸ƒ:")
                for step, count in sorted(error_stats['fail_steps'].items(), 
                                         key=lambda x: x[1], reverse=True)[:3]:
                    logger.info(f"    - {step}: {count} æ¬¡")
        
        logger.info(f"ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_dir}")
        logger.info("="*70)
        
        return stats
    
    def _build_system_prompt(self, prompt_data: Dict, questions: List[Dict]) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨baseæç¤ºè¯
        return prompt_data.get('base', '')
    
    def _calculate_stats(self, results: List[Dict]) -> Dict[str, Any]:
        """
        è®¡ç®—è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
        
        åŒ…æ‹¬ï¼š
        - åŸºæœ¬æŒ‡æ ‡ï¼šæ€»æ•°ã€é€šè¿‡æ•°ã€å¤±è´¥æ•°ã€é€šè¿‡ç‡
        - å·¥å…·è°ƒç”¨ç»Ÿè®¡ï¼šæ€»æ¬¡æ•°ã€æˆåŠŸç‡ã€å„å·¥å…·ä½¿ç”¨æ¬¡æ•°
        - å¯¹è¯è½®æ¬¡ç»Ÿè®¡ï¼šå¹³å‡è½®æ¬¡ã€æœ€å¤§è½®æ¬¡
        - è¾“å‡ºç»Ÿè®¡ï¼šæ€»å­—ç¬¦æ•°ã€å¹³å‡å­—ç¬¦æ•°
        - æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„ç»Ÿè®¡
        """
        total = len(results)
        passed = sum(1 for r in results if r.get('pass', False))
        failed = total - passed
        
        # å·¥å…·è°ƒç”¨ç»Ÿè®¡
        tool_stats = {
            'total_calls': 0,
            'tool_types': {},  # æ¯ç§å·¥å…·çš„ä½¿ç”¨æ¬¡æ•°
            'avg_calls_per_task': 0
        }
        
        # å¯¹è¯è½®æ¬¡ç»Ÿè®¡
        round_stats = {
            'total_rounds': 0,
            'avg_rounds': 0,
            'max_rounds': 0,
            'min_rounds': float('inf')
        }
        
        # è¾“å‡ºç»Ÿè®¡
        output_stats = {
            'total_chars': 0,
            'avg_chars_per_task': 0,
            'max_chars': 0,
            'min_chars': float('inf')
        }
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        by_task_type = {}
        
        # é”™è¯¯ç»Ÿè®¡
        error_stats = {
            'total_errors': 0,
            'error_types': {},
            'fail_steps': {}
        }
        
        for result in results:
            task_type = result.get('tag', 'unknown')
            
            # åˆå§‹åŒ–ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            if task_type not in by_task_type:
                by_task_type[task_type] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'pass_rate': 0,
                    'avg_rounds': 0,
                    'total_rounds': 0,
                    'tool_calls': 0,
                    'total_output_chars': 0
                }
            
            by_task_type[task_type]['total'] += 1
            if result.get('pass', False):
                by_task_type[task_type]['passed'] += 1
            else:
                by_task_type[task_type]['failed'] += 1
            
            # ä½¿ç”¨å·²ç»è®¡ç®—å¥½çš„ metrics
            metrics = result.get('metrics', {})
            
            # ç»Ÿè®¡å·¥å…·è°ƒç”¨ï¼ˆä½¿ç”¨ metricsï¼‰
            tool_calls_count = metrics.get('tool_calls', 0)
            tool_stats['total_calls'] += tool_calls_count
            by_task_type[task_type]['tool_calls'] += tool_calls_count
            
            # ä» metrics ä¸­è·å–å·¥å…·ç±»å‹ç»Ÿè®¡
            for tool_name, count in metrics.get('tool_types', {}).items():
                if tool_name not in tool_stats['tool_types']:
                    tool_stats['tool_types'][tool_name] = 0
                tool_stats['tool_types'][tool_name] += count
            
            # ç»Ÿè®¡å¯¹è¯è½®æ¬¡ï¼ˆä½¿ç”¨ metricsï¼‰
            round_count = metrics.get('total_rounds', 0)
            if round_count > 0:
                round_stats['total_rounds'] += round_count
                round_stats['max_rounds'] = max(round_stats['max_rounds'], round_count)
                round_stats['min_rounds'] = min(round_stats['min_rounds'], round_count)
                by_task_type[task_type]['total_rounds'] += round_count
            
            # ç»Ÿè®¡è¾“å‡ºå­—ç¬¦æ•°ï¼ˆä½¿ç”¨ metricsï¼‰
            output_chars = metrics.get('output_chars', 0)
            if output_chars > 0:
                output_stats['total_chars'] += output_chars
                output_stats['max_chars'] = max(output_stats['max_chars'], output_chars)
                output_stats['min_chars'] = min(output_stats['min_chars'], output_chars)
                by_task_type[task_type]['total_output_chars'] += output_chars
            
            # ç»Ÿè®¡é”™è¯¯
            if not result.get('pass', False):
                error_stats['total_errors'] += 1
                
                error_type = result.get('error_type', 'Unknown')
                if error_type not in error_stats['error_types']:
                    error_stats['error_types'][error_type] = 0
                error_stats['error_types'][error_type] += 1
                
                fail_step = result.get('fail_step', 'Unknown')
                if fail_step not in error_stats['fail_steps']:
                    error_stats['fail_steps'][fail_step] = 0
                error_stats['fail_steps'][fail_step] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        if total > 0:
            tool_stats['avg_calls_per_task'] = tool_stats['total_calls'] / total
            round_stats['avg_rounds'] = round_stats['total_rounds'] / total
            output_stats['avg_chars_per_task'] = output_stats['total_chars'] / total
        
        if round_stats['min_rounds'] == float('inf'):
            round_stats['min_rounds'] = 0
        if output_stats['min_chars'] == float('inf'):
            output_stats['min_chars'] = 0
        
        # è®¡ç®—å„ä»»åŠ¡ç±»å‹çš„å¹³å‡å€¼å’Œé€šè¿‡ç‡
        for task_type in by_task_type:
            stats = by_task_type[task_type]
            if stats['total'] > 0:
                stats['pass_rate'] = stats['passed'] / stats['total']
                stats['avg_rounds'] = stats['total_rounds'] / stats['total']
                stats['avg_output_chars'] = stats['total_output_chars'] / stats['total']
        
        return {
            # åŸºæœ¬ç»Ÿè®¡
            'total': total,
            'passed': passed,
            'failed': failed,
            'pass_rate': passed / total if total > 0 else 0,
            
            # å·¥å…·è°ƒç”¨ç»Ÿè®¡
            'tool_stats': tool_stats,
            
            # å¯¹è¯è½®æ¬¡ç»Ÿè®¡
            'round_stats': round_stats,
            
            # è¾“å‡ºç»Ÿè®¡
            'output_stats': output_stats,
            
            # é”™è¯¯ç»Ÿè®¡
            'error_stats': error_stats,
            
            # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            'by_task_type': by_task_type,
            
            # åŸå§‹ç»“æœ
            'results': results
        }


if __name__ == "__main__":
    # æµ‹è¯•
    engine = EvaluationEngine()
    logger.info("è¯„æµ‹å¼•æ“å·²åˆ›å»º")

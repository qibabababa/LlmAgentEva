#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§æ¨¡å—
æ”¶é›†å’Œç»Ÿè®¡è¯„æµ‹è¿‡ç¨‹ä¸­çš„å„ç§æŒ‡æ ‡
"""

import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict, field
from datetime import datetime
from threading import Lock

from lib.core.logger import get_logger

logger = get_logger(__name__)


@dataclass
class APICallMetric:
    """APIè°ƒç”¨æŒ‡æ ‡"""
    timestamp: float
    model: str
    latency: float  # ç§’
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    success: bool = True
    error: Optional[str] = None
    task_id: Optional[str] = None


@dataclass
class TaskMetric:
    """ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡"""
    task_id: str
    task_type: str
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    success: bool = False
    api_calls: int = 0
    tool_calls: int = 0
    total_tokens: int = 0
    error: Optional[str] = None


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    _instance = None
    _lock = Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨"""
        if not hasattr(self, '_initialized'):
            self.api_metrics: List[APICallMetric] = []
            self.task_metrics: List[TaskMetric] = []
            self.current_tasks: Dict[str, TaskMetric] = {}
            self._initialized = True
            logger.info("æ€§èƒ½ç›‘æ§å·²åˆå§‹åŒ–")
    
    def record_api_call(self,
                       model: str,
                       latency: float,
                       prompt_tokens: int = 0,
                       completion_tokens: int = 0,
                       success: bool = True,
                       error: Optional[str] = None,
                       task_id: Optional[str] = None):
        """
        è®°å½•APIè°ƒç”¨
        
        Args:
            model: æ¨¡å‹åç§°
            latency: å»¶è¿Ÿï¼ˆç§’ï¼‰
            prompt_tokens: æç¤ºè¯tokenæ•°
            completion_tokens: å®Œæˆtokenæ•°
            success: æ˜¯å¦æˆåŠŸ
            error: é”™è¯¯ä¿¡æ¯
            task_id: å…³è”çš„ä»»åŠ¡ID
        """
        metric = APICallMetric(
            timestamp=time.time(),
            model=model,
            latency=latency,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens,
            success=success,
            error=error,
            task_id=task_id
        )
        
        with self._lock:
            self.api_metrics.append(metric)
            
            # æ›´æ–°ä»»åŠ¡æŒ‡æ ‡
            if task_id and task_id in self.current_tasks:
                self.current_tasks[task_id].api_calls += 1
                self.current_tasks[task_id].total_tokens += metric.total_tokens
        
        logger.debug(f"è®°å½•APIè°ƒç”¨: model={model}, latency={latency:.2f}s, tokens={metric.total_tokens}")
    
    def start_task(self, task_id: str, task_type: str):
        """
        å¼€å§‹ä»»åŠ¡è®¡æ—¶
        
        Args:
            task_id: ä»»åŠ¡ID
            task_type: ä»»åŠ¡ç±»å‹
        """
        metric = TaskMetric(
            task_id=task_id,
            task_type=task_type,
            start_time=time.time()
        )
        
        with self._lock:
            self.current_tasks[task_id] = metric
        
        logger.debug(f"å¼€å§‹ä»»åŠ¡: {task_id} ({task_type})")
    
    def end_task(self, task_id: str, success: bool, error: Optional[str] = None):
        """
        ç»“æŸä»»åŠ¡è®¡æ—¶
        
        Args:
            task_id: ä»»åŠ¡ID
            success: æ˜¯å¦æˆåŠŸ
            error: é”™è¯¯ä¿¡æ¯
        """
        with self._lock:
            if task_id not in self.current_tasks:
                logger.warning(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return
            
            metric = self.current_tasks[task_id]
            metric.end_time = time.time()
            metric.duration = metric.end_time - metric.start_time
            metric.success = success
            metric.error = error
            
            # ç§»åŠ¨åˆ°å®Œæˆåˆ—è¡¨
            self.task_metrics.append(metric)
            del self.current_tasks[task_id]
        
        logger.debug(f"å®Œæˆä»»åŠ¡: {task_id}, è€—æ—¶={metric.duration:.2f}s, æˆåŠŸ={success}")
    
    def record_tool_call(self, task_id: str):
        """
        è®°å½•å·¥å…·è°ƒç”¨
        
        Args:
            task_id: ä»»åŠ¡ID
        """
        with self._lock:
            if task_id in self.current_tasks:
                self.current_tasks[task_id].tool_calls += 1
    
    def get_summary(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡æ‘˜è¦
        
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        with self._lock:
            # APIæŒ‡æ ‡ç»Ÿè®¡
            api_count = len(self.api_metrics)
            api_success_count = sum(1 for m in self.api_metrics if m.success)
            api_success_rate = api_success_count / api_count if api_count > 0 else 0
            
            total_latency = sum(m.latency for m in self.api_metrics)
            avg_latency = total_latency / api_count if api_count > 0 else 0
            
            total_tokens = sum(m.total_tokens for m in self.api_metrics)
            total_prompt_tokens = sum(m.prompt_tokens for m in self.api_metrics)
            total_completion_tokens = sum(m.completion_tokens for m in self.api_metrics)
            
            # ä»»åŠ¡æŒ‡æ ‡ç»Ÿè®¡
            task_count = len(self.task_metrics)
            task_success_count = sum(1 for m in self.task_metrics if m.success)
            task_success_rate = task_success_count / task_count if task_count > 0 else 0
            
            total_duration = sum(m.duration for m in self.task_metrics if m.duration)
            avg_duration = total_duration / task_count if task_count > 0 else 0
            
            avg_tokens_per_task = total_tokens / task_count if task_count > 0 else 0
            
            # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            task_type_stats = {}
            for metric in self.task_metrics:
                task_type = metric.task_type
                if task_type not in task_type_stats:
                    task_type_stats[task_type] = {
                        'count': 0,
                        'success': 0,
                        'failed': 0,
                        'total_tokens': 0,
                        'avg_duration': 0
                    }
                
                task_type_stats[task_type]['count'] += 1
                if metric.success:
                    task_type_stats[task_type]['success'] += 1
                else:
                    task_type_stats[task_type]['failed'] += 1
                task_type_stats[task_type]['total_tokens'] += metric.total_tokens
                if metric.duration:
                    task_type_stats[task_type]['avg_duration'] += metric.duration
            
            # è®¡ç®—å¹³å‡å€¼
            for task_type in task_type_stats:
                count = task_type_stats[task_type]['count']
                if count > 0:
                    task_type_stats[task_type]['avg_duration'] /= count
                    task_type_stats[task_type]['success_rate'] = \
                        task_type_stats[task_type]['success'] / count
            
            summary = {
                'timestamp': datetime.now().isoformat(),
                
                # APIç»Ÿè®¡
                'api': {
                    'total_calls': api_count,
                    'success_calls': api_success_count,
                    'failed_calls': api_count - api_success_count,
                    'success_rate': api_success_rate,
                    'avg_latency': avg_latency,
                    'total_latency': total_latency,
                },
                
                # Tokenç»Ÿè®¡
                'tokens': {
                    'total': total_tokens,
                    'prompt': total_prompt_tokens,
                    'completion': total_completion_tokens,
                    'avg_per_task': avg_tokens_per_task,
                },
                
                # ä»»åŠ¡ç»Ÿè®¡
                'tasks': {
                    'total': task_count,
                    'success': task_success_count,
                    'failed': task_count - task_success_count,
                    'success_rate': task_success_rate,
                    'avg_duration': avg_duration,
                    'total_duration': total_duration,
                },
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                'by_task_type': task_type_stats,
            }
            
            return summary
    
    def export_to_json(self, output_file: Path):
        """
        å¯¼å‡ºæŒ‡æ ‡åˆ°JSONæ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        summary = self.get_summary()
        
        # æ·»åŠ è¯¦ç»†æŒ‡æ ‡
        detailed = {
            'summary': summary,
            'api_calls': [asdict(m) for m in self.api_metrics],
            'tasks': [asdict(m) for m in self.task_metrics],
        }
        
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²å¯¼å‡º: {output_file}")
    
    def export_to_prometheus(self) -> str:
        """
        å¯¼å‡ºä¸ºPrometheusæ ¼å¼
        
        Returns:
            Prometheusæ ¼å¼çš„æŒ‡æ ‡æ–‡æœ¬
        """
        summary = self.get_summary()
        
        lines = [
            "# HELP evaluation_api_calls_total Total number of API calls",
            "# TYPE evaluation_api_calls_total counter",
            f"evaluation_api_calls_total {summary['api']['total_calls']}",
            "",
            "# HELP evaluation_api_success_rate API call success rate",
            "# TYPE evaluation_api_success_rate gauge",
            f"evaluation_api_success_rate {summary['api']['success_rate']:.4f}",
            "",
            "# HELP evaluation_api_latency_seconds Average API latency in seconds",
            "# TYPE evaluation_api_latency_seconds gauge",
            f"evaluation_api_latency_seconds {summary['api']['avg_latency']:.4f}",
            "",
            "# HELP evaluation_tokens_total Total tokens used",
            "# TYPE evaluation_tokens_total counter",
            f"evaluation_tokens_total {summary['tokens']['total']}",
            "",
            "# HELP evaluation_tasks_total Total number of tasks",
            "# TYPE evaluation_tasks_total counter",
            f"evaluation_tasks_total {summary['tasks']['total']}",
            "",
            "# HELP evaluation_task_success_rate Task success rate",
            "# TYPE evaluation_task_success_rate gauge",
            f"evaluation_task_success_rate {summary['tasks']['success_rate']:.4f}",
            "",
        ]
        
        return "\n".join(lines)
    
    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        summary = self.get_summary()
        
        print("\n" + "=" * 70)
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
        print("=" * 70)
        
        # APIç»Ÿè®¡
        api = summary['api']
        print(f"\nAPIè°ƒç”¨:")
        print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {api['total_calls']}")
        print(f"  æˆåŠŸ: {api['success_calls']}")
        print(f"  å¤±è´¥: {api['failed_calls']}")
        print(f"  æˆåŠŸç‡: {api['success_rate']:.1%}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {api['avg_latency']:.2f}ç§’")
        print(f"  æ€»è€—æ—¶: {api['total_latency']:.2f}ç§’")
        
        # Tokenç»Ÿè®¡
        tokens = summary['tokens']
        print(f"\nTokenä½¿ç”¨:")
        print(f"  æ€»Token: {tokens['total']:,}")
        print(f"  æç¤ºè¯Token: {tokens['prompt']:,}")
        print(f"  å®ŒæˆToken: {tokens['completion']:,}")
        print(f"  å¹³å‡Token/ä»»åŠ¡: {tokens['avg_per_task']:.0f}")
        
        # ä»»åŠ¡ç»Ÿè®¡
        tasks = summary['tasks']
        print(f"\nä»»åŠ¡æ‰§è¡Œ:")
        print(f"  æ€»ä»»åŠ¡æ•°: {tasks['total']}")
        print(f"  æˆåŠŸ: {tasks['success']}")
        print(f"  å¤±è´¥: {tasks['failed']}")
        print(f"  æˆåŠŸç‡: {tasks['success_rate']:.1%}")
        print(f"  å¹³å‡è€—æ—¶: {tasks['avg_duration']:.2f}ç§’")
        print(f"  æ€»è€—æ—¶: {tasks['total_duration']:.2f}ç§’")
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        if summary['by_task_type']:
            print(f"\næŒ‰ä»»åŠ¡ç±»å‹:")
            for task_type, stats in summary['by_task_type'].items():
                print(f"  {task_type}:")
                print(f"    æ•°é‡: {stats['count']}")
                print(f"    æˆåŠŸç‡: {stats['success_rate']:.1%}")
                print(f"    å¹³å‡è€—æ—¶: {stats['avg_duration']:.2f}ç§’")
                print(f"    Tokenä½¿ç”¨: {stats['total_tokens']:,}")
        
        print("=" * 70 + "\n")
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        with self._lock:
            self.api_metrics.clear()
            self.task_metrics.clear()
            self.current_tasks.clear()
        
        logger.info("æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®")


# å…¨å±€å•ä¾‹
_metrics_collector = None


def get_metrics_collector() -> MetricsCollector:
    """è·å–å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹"""
    global _metrics_collector
    if _metrics_collector is None:
        _metrics_collector = MetricsCollector()
    return _metrics_collector


if __name__ == "__main__":
    # æµ‹è¯•
    collector = MetricsCollector()
    
    # æ¨¡æ‹Ÿä»»åŠ¡
    collector.start_task("task_1", "fix_bug")
    collector.record_api_call("qwen3-235b", 2.5, 100, 50, True, task_id="task_1")
    time.sleep(0.1)
    collector.record_api_call("qwen3-235b", 1.8, 80, 40, True, task_id="task_1")
    collector.record_tool_call("task_1")
    collector.end_task("task_1", True)
    
    collector.start_task("task_2", "convert")
    collector.record_api_call("qwen3-235b", 3.0, 120, 60, False, "Timeout", task_id="task_2")
    collector.end_task("task_2", False, "API failed")
    
    # æ‰“å°æ‘˜è¦
    collector.print_summary()
    
    # å¯¼å‡º
    collector.export_to_json(Path("test_metrics.json"))
    print("\nâœ… æµ‹è¯•å®Œæˆ")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‰é˜¶æ®µè¯„æµ‹ç³»ç»Ÿ - äº¤äº’å¼ä¸»å…¥å£

æ”¯æŒè¯„æµ‹ä¸‰ä¸ªé˜¶æ®µ:
1. ä»»åŠ¡åˆ†è§£ (Task Decomposition) - è¯„ä¼°å¬å›ç‡å’Œå‡†ç¡®ç‡
2. ä»»åŠ¡è§„åˆ’ (Task Planning) - è¯„ä¼°è¦†ç›–åº¦å’Œé¡ºåºæ­£ç¡®æ€§
3. ä»»åŠ¡æ‰§è¡Œ (Task Execution) - è¯„ä¼°æœ€ç»ˆç»“æœæ˜¯å¦é€šè¿‡

ç”¨æˆ·å¯ä»¥é€‰æ‹©è¯„æµ‹å•ä¸ªé˜¶æ®µã€å¤šä¸ªé˜¶æ®µæˆ–å…¨æµç¨‹
"""

import sys
import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ libç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from lib.core.config_manager import get_config
from lib.core.logger import LoggerManager, get_logger
from lib.validators.task_decomposition import validate_task_decomposition
from lib.validators.task_planning import validate_task_planning
from lib.core.evaluation_engine import EvaluationEngine
from lib.api.client import APIClient


def print_banner():
    print("\n" + "="*70)
    print("                    ä¸‰é˜¶æ®µè¯„æµ‹ç³»ç»Ÿ v1.0")
    print("="*70)
    print("\næœ¬ç³»ç»Ÿæ”¯æŒä¸‰ä¸ªè¯„æµ‹é˜¶æ®µ:")
    print("  1ï¸âƒ£  ä»»åŠ¡åˆ†è§£ (Task Decomposition) - è¯„ä¼°ä»»åŠ¡æ‹†åˆ†èƒ½åŠ›")
    print("  2ï¸âƒ£  ä»»åŠ¡è§„åˆ’ (Task Planning) - è¯„ä¼°ä»»åŠ¡æ’åºå’Œä¾èµ–ç®¡ç†")
    print("  3ï¸âƒ£  ä»»åŠ¡æ‰§è¡Œ (Task Execution) - è¯„ä¼°æœ€ç»ˆæ‰§è¡Œç»“æœ")
    print("="*70 + "\n")


def select_stages() -> List[str]:
    """
    äº¤äº’å¼é€‰æ‹©è¦è¯„æµ‹çš„é˜¶æ®µ
    
    Returns:
        é€‰ä¸­çš„é˜¶æ®µåˆ—è¡¨
    """
    print("è¯·é€‰æ‹©è¦è¯„æµ‹çš„é˜¶æ®µï¼ˆå¯å¤šé€‰ï¼‰:")
    print("  [1] ä»»åŠ¡åˆ†è§£ (Decomposition)")
    print("  [2] ä»»åŠ¡è§„åˆ’ (Planning)")
    print("  [3] ä»»åŠ¡æ‰§è¡Œ (Execution)")
    print("  [4] å…¨æµç¨‹ (All)")
    print("  [0] é€€å‡º\n")
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (ä¾‹å¦‚: 1 æˆ– 123 æˆ– 4): ").strip()
        
        if choice == '0':
            print("é€€å‡ºè¯„æµ‹ç³»ç»Ÿã€‚")
            sys.exit(0)
        
        if choice == '4':
            return ['decomposition', 'planning', 'execution']
        
        stages = []
        for c in choice:
            if c == '1':
                stages.append('decomposition')
            elif c == '2':
                stages.append('planning')
            elif c == '3':
                stages.append('execution')
        
        if stages:
            return stages
        
        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚\n")


def run_decomposition_evaluation(test_cases: List[Dict[str, Any]], model: str = None) -> Dict[str, Any]:
    """
    è¿è¡Œä»»åŠ¡åˆ†è§£è¯„æµ‹
    
    Args:
        test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è¯„æµ‹ç»“æœç»Ÿè®¡
    """
    logger = get_logger(__name__)
    logger.info("å¼€å§‹ä»»åŠ¡åˆ†è§£è¯„æµ‹")
    
    print("\n" + "="*70)
    print("é˜¶æ®µ 1: ä»»åŠ¡åˆ†è§£è¯„æµ‹")
    print("="*70)
    
    # åˆå§‹åŒ– API å®¢æˆ·ç«¯
    config = get_config()
    client = APIClient(model=model or config.api.default_model)
    print(f"ä½¿ç”¨æ¨¡å‹: {client.model}\n")
    
    # åŠ è½½ system_prompt_2.json
    from lib.core.utils import read_json
    system_prompt_file = config.paths.prompts_dir / "system_prompt_2.json"
    prompt_data = read_json(system_prompt_file)
    
    # æ„å»ºä»»åŠ¡åˆ†è§£çš„ç³»ç»Ÿæç¤ºè¯
    base_prompt = prompt_data.get('base', '')
    task_decomp = prompt_data.get('task', {}).get('task_decomposition', {})
    decomp_base = task_decomp.get('base', '')
    
    # ä»é…ç½®è¯»å–é»˜è®¤æ ¼å¼
    default_format = config.get('prompts.stages.decomposition.default_format', 'markdown')
    logger.info(f"ä»»åŠ¡åˆ†è§£é»˜è®¤æ ¼å¼: {default_format}")
    
    # ä»é…ç½®è¯»å–ç›¸ä¼¼åº¦åˆ¤æ–­è®¾ç½®
    use_llm_similarity = config.get('evaluation.task_decomposition.use_llm_similarity', True)
    similarity_threshold = config.get('evaluation.task_decomposition.similarity_threshold', 0.7)
    
    logger.info(f"ä½¿ç”¨LLMè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­: {use_llm_similarity}, é˜ˆå€¼: {similarity_threshold}")
    
    results = []
    total_recall = 0.0
    total_precision = 0.0
    total_f1 = 0.0
    
    for i, case in enumerate(test_cases):
        if case.get('stage') != 'decomposition':
            continue
        
        print(f"\n[{i+1}] è¯„æµ‹ç”¨ä¾‹: {case.get('name', f'Case {i+1}')}")
        
        # æ ¹æ®æµ‹è¯•ç”¨ä¾‹çš„ mode å’Œ format é€‰æ‹©åˆé€‚çš„æç¤ºè¯
        case_mode = case.get('mode', 'open')
        case_format = case.get('format', default_format)  # æµ‹è¯•ç”¨ä¾‹å¯ä»¥æŒ‡å®šæ ¼å¼
        
        logger.info(f"ç”¨ä¾‹æ¨¡å¼: {case_mode}, è¾“å‡ºæ ¼å¼: {case_format}")
        
        # æ ¹æ® mode é€‰æ‹©åŸºç¡€æç¤ºè¯
        if case_mode == 'constrained':
            # å…¨é›†æ¨¡å¼ï¼šä»æä¾›çš„ä»»åŠ¡é›†åˆä¸­é€‰æ‹©
            mode_base = task_decomp.get('all_tasks', decomp_base)
            format_section = task_decomp.get('format_all', {})
        else:
            # å¼€æ”¾æ¨¡å¼ï¼šè‡ªç”±åˆ†è§£
            mode_base = decomp_base
            format_section = task_decomp.get('format', {})
        
        # è·å–æ ¼å¼æ¨¡æ¿
        format_base = format_section.get('base', '\nè¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n')
        format_template = format_section.get(case_format, '')
        
        if not format_template:
            logger.warning(f"æœªæ‰¾åˆ°æ ¼å¼ {case_format} çš„æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼")
            format_template = format_section.get(default_format, '')
        
        # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = base_prompt + "\n" + mode_base + format_base + format_template
        
        # ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹ä¸­çš„ user_question
        user_question = case.get('user_question', '')
        
        logger.info(f"æ­£åœ¨è°ƒç”¨æ¨¡å‹è¿›è¡Œä»»åŠ¡åˆ†è§£: {case.get('name')}")
        
        try:
            # è°ƒç”¨ API è·å–æ¨¡å‹å›å¤
            response = client.chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_question}
                ]
            )
            
            model_response = response['choices'][0]['message']['content']
            logger.info(f"æ¨¡å‹åŸå§‹å›å¤:\n{model_response}")
            
        except Exception as e:
            logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
            print(f"  âŒ API è°ƒç”¨å¤±è´¥: {e}")
            continue
        
        # éªŒè¯ç»“æœ - ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
        result = validate_task_decomposition(
            model_response=model_response,
            ground_truth=case['ground_truth'],
            mode=case_mode,
            format_type=case_format,
            similarity_threshold=similarity_threshold,
            use_llm_similarity=use_llm_similarity
        )
        
        # è®°å½•è¯¦ç»†çš„æå–å’ŒéªŒè¯ä¿¡æ¯
        logger.info(f"æå–çš„ä»»åŠ¡æ•°é‡: {result.get('num_model_output', 0)}")
        logger.info(f"æå–çš„ä»»åŠ¡åˆ—è¡¨: {result.get('model_tasks', [])}")
        logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {case['ground_truth']}")
        logger.info(f"åŒ¹é…çš„ä»»åŠ¡å¯¹: {result.get('matched_pairs', [])}")
        logger.info(f"é—æ¼çš„ä»»åŠ¡: {result.get('missed_tasks', [])}")
        logger.info(f"å¤šä½™çš„ä»»åŠ¡: {result.get('extra_tasks', [])}")
        
        print(f"  å¬å›ç‡ (Recall):    {result['recall']:.2%}")
        print(f"  å‡†ç¡®ç‡ (Precision): {result['precision']:.2%}")
        print(f"  F1 åˆ†æ•°:           {result['f1_score']:.2%}")
        
        if result.get('missed_tasks'):
            print(f"  âš ï¸  é—æ¼ä»»åŠ¡: {len(result['missed_tasks'])} ä¸ª")
        
        # ä¿å­˜æ¨¡å‹å›å¤åˆ°ç»“æœä¸­
        result['model_response'] = model_response
        result['case_name'] = case.get('name')
        
        results.append(result)
        total_recall += result['recall']
        total_precision += result['precision']
        total_f1 += result['f1_score']
        
        logger.info(f"ç”¨ä¾‹ {i+1}: å¬å›={result['recall']:.2%}, å‡†ç¡®={result['precision']:.2%}")
    
    num_cases = len(results)
    if num_cases == 0:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡åˆ†è§£çš„æµ‹è¯•ç”¨ä¾‹")
        return {}
    
    summary = {
        'stage': 'decomposition',
        'total_cases': num_cases,
        'avg_recall': total_recall / num_cases,
        'avg_precision': total_precision / num_cases,
        'avg_f1_score': total_f1 / num_cases,
        'results': results
    }
    
    print("\n" + "-"*70)
    print("ğŸ“Š ä»»åŠ¡åˆ†è§£è¯„æµ‹æ±‡æ€»:")
    print(f"  æµ‹è¯•ç”¨ä¾‹æ•°: {num_cases}")
    print(f"  å¹³å‡å¬å›ç‡: {summary['avg_recall']:.2%}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {summary['avg_precision']:.2%}")
    print(f"  å¹³å‡F1åˆ†æ•°: {summary['avg_f1_score']:.2%}")
    
    logger.info(f"ä»»åŠ¡åˆ†è§£è¯„æµ‹å®Œæˆ: å¹³å‡F1={summary['avg_f1_score']:.2%}")
    
    return summary


def run_planning_evaluation(test_cases: List[Dict[str, Any]], model: str = None) -> Dict[str, Any]:
    """
    è¿è¡Œä»»åŠ¡è§„åˆ’è¯„æµ‹
    
    Args:
        test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è¯„æµ‹ç»“æœç»Ÿè®¡
    """
    logger = get_logger(__name__)
    logger.info("å¼€å§‹ä»»åŠ¡è§„åˆ’è¯„æµ‹")
    
    print("\n" + "="*70)
    print("é˜¶æ®µ 2: ä»»åŠ¡è§„åˆ’è¯„æµ‹")
    print("="*70)
    
    # åˆå§‹åŒ– API å®¢æˆ·ç«¯
    config = get_config()
    client = APIClient(model=model or config.api.default_model)
    print(f"ä½¿ç”¨æ¨¡å‹: {client.model}\n")
    
    # åŠ è½½ system_prompt_2.json
    from lib.core.utils import read_json
    system_prompt_file = config.paths.prompts_dir / "system_prompt_2.json"
    prompt_data = read_json(system_prompt_file)
    
    # æ„å»ºä»»åŠ¡è§„åˆ’çš„ç³»ç»Ÿæç¤ºè¯
    base_prompt = prompt_data.get('base', '')
    task_planning = prompt_data.get('task', {}).get('task_planning', {})
    planning_base = task_planning.get('base', '')
    
    system_prompt = base_prompt + "\n" + planning_base
    
    results = []
    total_coverage = 0.0
    total_order = 0.0
    total_overall = 0.0
    
    for i, case in enumerate(test_cases):
        if case.get('stage') != 'planning':
            continue
        
        print(f"\n[{i+1}] è¯„æµ‹ç”¨ä¾‹: {case.get('name', f'Case {i+1}')}")
        
        # æ„é€ ç”¨æˆ·æ¶ˆæ¯ï¼šç»™å‡ºä»»åŠ¡åˆ—è¡¨
        tasks = case.get('ground_truth_tasks', [])
        user_message = "å·²æ‹†è§£å¥½çš„å­ä»»åŠ¡åˆ—è¡¨ï¼š\n" + "\n".join([f"- {task}" for task in tasks])
        
        logger.info(f"æ­£åœ¨è°ƒç”¨æ¨¡å‹è¿›è¡Œä»»åŠ¡è§„åˆ’: {case.get('name')}")
        
        try:
            # è°ƒç”¨ API è·å–æ¨¡å‹å›å¤
            response = client.chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ]
            )
            
            model_response = response['choices'][0]['message']['content']
            logger.debug(f"æ¨¡å‹å›å¤: {model_response[:200]}...")
            
        except Exception as e:
            logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
            print(f"  âŒ API è°ƒç”¨å¤±è´¥: {e}")
            continue
        
        # éªŒè¯ç»“æœ
        result = validate_task_planning(
            model_response=model_response,
            ground_truth_tasks=case['ground_truth_tasks'],
            dependencies=case.get('dependencies')
        )
        
        print(f"  è¦†ç›–åº¦ (Coverage):          {result['coverage']:.2%}")
        print(f"  é¡ºåºæ­£ç¡®ç‡ (Order):         {result['order_correctness']:.2%}")
        print(f"  å±‚çº§æ•ˆç‡ (Efficiency):      {result['level_efficiency']:.2%}")
        print(f"  ç»¼åˆå¾—åˆ† (Overall):         {result['overall_score']:.2%}")
        
        violations = result.get('detailed_results', {}).get('order', {}).get('violations', [])
        if violations:
            print(f"  âš ï¸  ä¾èµ–è¿å: {len(violations)} ä¸ª")
        
        # ä¿å­˜æ¨¡å‹å›å¤åˆ°ç»“æœä¸­
        result['model_response'] = model_response
        result['case_name'] = case.get('name')
        
        results.append(result)
        total_coverage += result['coverage']
        total_order += result['order_correctness']
        total_overall += result['overall_score']
        
        logger.info(f"ç”¨ä¾‹ {i+1}: è¦†ç›–={result['coverage']:.2%}, é¡ºåº={result['order_correctness']:.2%}")
    
    num_cases = len(results)
    if num_cases == 0:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡è§„åˆ’çš„æµ‹è¯•ç”¨ä¾‹")
        return {}
    
    summary = {
        'stage': 'planning',
        'total_cases': num_cases,
        'avg_coverage': total_coverage / num_cases,
        'avg_order_correctness': total_order / num_cases,
        'avg_overall_score': total_overall / num_cases,
        'results': results
    }
    
    print("\n" + "-"*70)
    print("ğŸ“Š ä»»åŠ¡è§„åˆ’è¯„æµ‹æ±‡æ€»:")
    print(f"  æµ‹è¯•ç”¨ä¾‹æ•°: {num_cases}")
    print(f"  å¹³å‡è¦†ç›–åº¦: {summary['avg_coverage']:.2%}")
    print(f"  å¹³å‡é¡ºåºæ­£ç¡®ç‡: {summary['avg_order_correctness']:.2%}")
    print(f"  å¹³å‡ç»¼åˆå¾—åˆ†: {summary['avg_overall_score']:.2%}")
    
    logger.info(f"ä»»åŠ¡è§„åˆ’è¯„æµ‹å®Œæˆ: å¹³å‡å¾—åˆ†={summary['avg_overall_score']:.2%}")
    
    return summary


def run_execution_evaluation(task_type: str, model: str, use_stream: bool) -> Dict[str, Any]:
    """
    è¿è¡Œä»»åŠ¡æ‰§è¡Œè¯„æµ‹
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹
        model: æ¨¡å‹åç§°
        use_stream: æ˜¯å¦ä½¿ç”¨æµå¼API
        
    Returns:
        è¯„æµ‹ç»“æœç»Ÿè®¡
    """
    logger = get_logger(__name__)
    logger.info("å¼€å§‹ä»»åŠ¡æ‰§è¡Œè¯„æµ‹")
    
    print("\n" + "="*70)
    print("é˜¶æ®µ 3: ä»»åŠ¡æ‰§è¡Œè¯„æµ‹")
    print("="*70)
    
    print(f"\nä»»åŠ¡ç±»å‹: {task_type}")
    print(f"æ¨¡å‹: {model}")
    print(f"æµå¼æ¨¡å¼: {'å¯ç”¨' if use_stream else 'ç¦ç”¨'}")
    
    # åˆ›å»ºè¯„æµ‹å¼•æ“
    engine = EvaluationEngine(model=model, use_stream=use_stream)
    
    # è¿è¡Œè¯„æµ‹
    stats = engine.run_evaluation(task_type=task_type)
    
    print("\n" + "-"*70)
    print("ğŸ“Š ä»»åŠ¡æ‰§è¡Œè¯„æµ‹æ±‡æ€»:")
    print(f"  æ€»ä»»åŠ¡æ•°: {stats['total']}")
    print(f"  é€šè¿‡: {stats['passed']} âœ“")
    print(f"  å¤±è´¥: {stats['failed']} âœ—")
    print(f"  é€šè¿‡ç‡: {stats['pass_rate']:.1%}")
    
    logger.info(f"ä»»åŠ¡æ‰§è¡Œè¯„æµ‹å®Œæˆ: é€šè¿‡ç‡={stats['pass_rate']:.1%}")
    
    return stats


def load_test_cases(file_path: str) -> List[Dict[str, Any]]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict):
            return [data]
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
        return []


def save_results(results: Dict[str, Any], output_path: str):
    """
    ä¿å­˜è¯„æµ‹ç»“æœåˆ°JSONæ–‡ä»¶
    
    Args:
        results: è¯„æµ‹ç»“æœ
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸‰é˜¶æ®µè¯„æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # äº¤äº’å¼é€‰æ‹©é˜¶æ®µ
  python bin/run_stage_evaluation.py
  
  # ç›´æ¥æŒ‡å®šé˜¶æ®µ
  python bin/run_stage_evaluation.py --stages decomposition planning
  
  # è¯„æµ‹å…¨æµç¨‹
  python bin/run_stage_evaluation.py --stages all
  
  # æŒ‡å®šæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶
  python bin/run_stage_evaluation.py --test-file data/test_cases.json
  
  # ä»»åŠ¡æ‰§è¡Œé˜¶æ®µæŒ‡å®šä»»åŠ¡ç±»å‹
  python bin/run_stage_evaluation.py --stages execution --task-type fix_bug
        """
    )
    
    parser.add_argument(
        "--stages",
        nargs='+',
        choices=['decomposition', 'planning', 'execution', 'all'],
        help="è¦è¯„æµ‹çš„é˜¶æ®µï¼ˆå¯å¤šä¸ªï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰"
    )
    
    parser.add_argument(
        "--test-file",
        help="æµ‹è¯•ç”¨ä¾‹JSONæ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--task-type",
        choices=["fix_bug", "convert", "refactor", "env", "sum", "split", "all"],
        default="all",
        help="ä»»åŠ¡æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡ç±»å‹ï¼ˆé»˜è®¤: allï¼‰"
    )
    
    parser.add_argument(
        "--model",
        help="æ¨¡å‹åç§°"
    )
    
    parser.add_argument(
        "--no-stream",
        action="store_true",
        help="ç¦ç”¨æµå¼API"
    )
    
    parser.add_argument(
        "--output",
        help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--non-interactive",
        action="store_true",
        help="éäº¤äº’æ¨¡å¼ï¼ˆå¿…é¡»æŒ‡å®š--stagesï¼‰"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # æ‰“å°æ¬¢è¿æ¨ªå¹…
    print_banner()
    
    # åŠ è½½é…ç½®
    try:
        config = get_config()
        LoggerManager.initialize(config.paths.logs_dir)
        logger = get_logger(__name__)
        logger.info("="*70)
        logger.info("ä¸‰é˜¶æ®µè¯„æµ‹ç³»ç»Ÿå¯åŠ¨")
        logger.info("="*70)
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ç¡®å®šè¦è¯„æµ‹çš„é˜¶æ®µ
    if args.stages:
        if 'all' in args.stages:
            selected_stages = ['decomposition', 'planning', 'execution']
        else:
            selected_stages = args.stages
    elif args.non_interactive:
        print("âŒ éäº¤äº’æ¨¡å¼å¿…é¡»æŒ‡å®š --stages å‚æ•°")
        return 1
    else:
        selected_stages = select_stages()
    
    print(f"\nâœ… å°†è¯„æµ‹ä»¥ä¸‹é˜¶æ®µ: {', '.join(selected_stages)}")
    logger.info(f"é€‰æ‹©çš„è¯„æµ‹é˜¶æ®µ: {selected_stages}")
    
    # å‡†å¤‡ç»“æœå®¹å™¨
    all_results = {
        'stages': selected_stages,
        'results': {}
    }
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼ˆç”¨äºä»»åŠ¡åˆ†è§£å’Œè§„åˆ’ï¼‰
    test_cases = []
    if 'decomposition' in selected_stages or 'planning' in selected_stages:
        if args.test_file:
            test_cases = load_test_cases(args.test_file)
            if not test_cases:
                print(f"âš ï¸  æœªèƒ½ä» {args.test_file} åŠ è½½æµ‹è¯•ç”¨ä¾‹")
        else:
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹è·¯å¾„
            default_file = config.paths.data_dir / "stage_test_cases.json"
            if default_file.exists():
                test_cases = load_test_cases(str(default_file))
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶: {default_file}")
                print(f"   è¯·ä½¿ç”¨ --test-file æŒ‡å®šæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
    
    # è¿è¡Œå„é˜¶æ®µè¯„æµ‹
    try:
        # è·å–æ¨¡å‹åç§°
        model = args.model or config.api.default_model
        
        # é˜¶æ®µ1: ä»»åŠ¡åˆ†è§£
        if 'decomposition' in selected_stages:
            decomp_results = run_decomposition_evaluation(test_cases, model=model)
            if decomp_results:
                all_results['results']['decomposition'] = decomp_results
        
        # é˜¶æ®µ2: ä»»åŠ¡è§„åˆ’
        if 'planning' in selected_stages:
            planning_results = run_planning_evaluation(test_cases, model=model)
            if planning_results:
                all_results['results']['planning'] = planning_results
        
        # é˜¶æ®µ3: ä»»åŠ¡æ‰§è¡Œ
        if 'execution' in selected_stages:
            use_stream = config.api.stream_enabled and not args.no_stream
            
            execution_results = run_execution_evaluation(
                task_type=args.task_type,
                model=model,
                use_stream=use_stream
            )
            all_results['results']['execution'] = execution_results
        
        # æ˜¾ç¤ºæ€»ä½“æ€»ç»“
        print("\n" + "="*70)
        print("ğŸ‰ è¯„æµ‹å®Œæˆï¼")
        print("="*70)
        
        print("\nğŸ“Š æ€»ä½“æ€»ç»“:")
        for stage in selected_stages:
            if stage in all_results['results']:
                result = all_results['results'][stage]
                
                if stage == 'decomposition':
                    print(f"\n  ä»»åŠ¡åˆ†è§£:")
                    print(f"    å¹³å‡F1åˆ†æ•°: {result['avg_f1_score']:.2%}")
                
                elif stage == 'planning':
                    print(f"\n  ä»»åŠ¡è§„åˆ’:")
                    print(f"    å¹³å‡ç»¼åˆå¾—åˆ†: {result['avg_overall_score']:.2%}")
                
                elif stage == 'execution':
                    print(f"\n  ä»»åŠ¡æ‰§è¡Œ:")
                    print(f"    é€šè¿‡ç‡: {result['pass_rate']:.1%}")
        
        # ä¿å­˜ç»“æœ
        if args.output:
            save_results(all_results, args.output)
        else:
            # é»˜è®¤ä¿å­˜åˆ°outputsç›®å½•
            import time
            timestamp = int(time.time())
            output_path = config.paths.outputs_dir / f"stage_eval_{timestamp}.json"
            save_results(all_results, str(output_path))
        
        logger.info("è¯„æµ‹å®Œæˆ")
        LoggerManager.shutdown()
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        logger.warning("è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        LoggerManager.shutdown()
        return 130
    
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        logger.error(f"è¯„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        LoggerManager.shutdown()
        return 1


if __name__ == "__main__":
    sys.exit(main())

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‰é˜¶æ®µè¿ç»­è¯„æµ‹ç³»ç»Ÿ

å¯¹åŒä¸€ä¸ªä»»åŠ¡è¿ç»­æ‰§è¡Œä¸‰ä¸ªé˜¶æ®µï¼š
1. ä»»åŠ¡åˆ†è§£ - å°†ç”¨æˆ·é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡
2. ä»»åŠ¡è§„åˆ’ - å¯¹å­ä»»åŠ¡è¿›è¡Œæ’åºå’Œè§„åˆ’ä¾èµ–å…³ç³»
3. ä»»åŠ¡æ‰§è¡Œ - æŒ‰ç…§è§„åˆ’æ‰§è¡Œä»»åŠ¡å¹¶éªŒè¯ç»“æœ

æ¯ä¸ªé˜¶æ®µçš„è¾“å…¥ä¾èµ–äºä¸Šä¸€ä¸ªé˜¶æ®µçš„è¾“å‡º
"""

import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple

# æ·»åŠ libç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from lib.core.config_manager import get_config
from lib.core.logger import LoggerManager, get_logger
from lib.core.utils import read_json
from lib.core.output_control import set_show_details, print_detail
from lib.api.client import APIClient
from lib.validators.task_decomposition import validate_task_decomposition, extract_tasks_from_response
from lib.validators.task_planning import validate_task_planning
from lib.core.evaluation_engine import EvaluationEngine


def create_default_plan_from_dependencies(
    tasks: List[str],
    dependencies: Dict[str, List[str]]
) -> List[List[str]]:
    """
    æ ¹æ®ä¾èµ–å…³ç³»åˆ›å»ºé»˜è®¤çš„æ‰§è¡Œè®¡åˆ’
    
    ä½¿ç”¨æ‹“æ‰‘æ’åºç®—æ³•
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨
        dependencies: ä¾èµ–å…³ç³» {task: [prerequisite_tasks]}
    
    Returns:
        åˆ†å±‚çš„æ‰§è¡Œè®¡åˆ’ [[level1_tasks], [level2_tasks], ...]
    """
    # æ„å»ºä¾èµ–å›¾
    task_deps = {task: dependencies.get(task, []) for task in tasks}
    
    # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„å±‚çº§
    task_levels = {}
    
    def get_task_level(task: str) -> int:
        if task in task_levels:
            return task_levels[task]
        
        deps = task_deps.get(task, [])
        if not deps:
            task_levels[task] = 0
            return 0
        
        max_dep_level = max([get_task_level(dep) for dep in deps])
        task_levels[task] = max_dep_level + 1
        return task_levels[task]
    
    # è®¡ç®—æ‰€æœ‰ä»»åŠ¡çš„å±‚çº§
    for task in tasks:
        get_task_level(task)
    
    # æŒ‰å±‚çº§åˆ†ç»„
    max_level = max(task_levels.values()) if task_levels else 0
    plan = [[] for _ in range(max_level + 1)]
    
    for task, level in task_levels.items():
        plan[level].append(task)
    
    return plan


def print_banner():
    print("\n" + "="*70)
    print("           ä¸‰é˜¶æ®µè¿ç»­è¯„æµ‹ç³»ç»Ÿ v1.0")
    print("="*70)
    print("\nå¯¹åŒä¸€ä¸ªä»»åŠ¡è¿ç»­æ‰§è¡Œä¸‰ä¸ªé˜¶æ®µï¼š")
    print("  ğŸ”¹ é˜¶æ®µ1: ä»»åŠ¡åˆ†è§£ - åˆ†è§£ç”¨æˆ·é—®é¢˜ä¸ºå­ä»»åŠ¡")
    print("  ğŸ”¹ é˜¶æ®µ2: ä»»åŠ¡è§„åˆ’ - è§„åˆ’å­ä»»åŠ¡çš„æ‰§è¡Œé¡ºåºå’Œä¾èµ–")
    print("  ğŸ”¹ é˜¶æ®µ3: ä»»åŠ¡æ‰§è¡Œ - æŒ‰ç…§è§„åˆ’æ‰§è¡Œå¹¶éªŒè¯ç»“æœ")
    print("\næ ¸å¿ƒé€»è¾‘:")
    print("  âœ“ è¯„æµ‹æ¨¡å‹è¾“å‡º - æ¯ä¸ªé˜¶æ®µéƒ½è¯„æµ‹æ¨¡å‹çš„å®é™…è¡¨ç°")
    print("  âœ“ ä¼ é€’ground_truth - ä¸‹ä¸€é˜¶æ®µä½¿ç”¨æ ‡å‡†ç­”æ¡ˆä½œä¸ºè¾“å…¥")
    print("  âœ“ ä¿è¯ä¸Šä¸‹æ–‡ç¨³å®š - é¿å…é”™è¯¯ä¿¡æ¯çš„è¿é”ä¼ æ’­")
    print("="*70 + "\n")


def run_decomposition_stage(
    test_case: Dict[str, Any],
    client: APIClient,
    config: Any
) -> Tuple[Dict[str, Any], List[str]]:
    """
    è¿è¡Œä»»åŠ¡åˆ†è§£é˜¶æ®µ
    
    Returns:
        (éªŒè¯ç»“æœ, æå–çš„ä»»åŠ¡åˆ—è¡¨)
    """
    logger = get_logger(__name__)
    logger.info("="*70)
    logger.info("é˜¶æ®µ1ï¼šä»»åŠ¡åˆ†è§£")
    logger.info("="*70)
    
    print("\nğŸ”¹ é˜¶æ®µ1ï¼šä»»åŠ¡åˆ†è§£")
    print("-" * 70)
    
    # åŠ è½½æç¤ºè¯
    system_prompt_file = config.paths.prompts_dir / "system_prompt_2.json"
    prompt_data = read_json(system_prompt_file)
    
    base_prompt = prompt_data.get('base', '')
    task_decomp = prompt_data.get('task', {}).get('task_decomposition', {})
    decomp_base = task_decomp.get('base', '')
    
    # è·å–æ ¼å¼é…ç½®
    default_format = config.get('prompts.stages.decomposition.default_format', 'markdown')
    case_format = test_case.get('format', default_format)
    
    format_section = task_decomp.get('format', {})
    format_base = format_section.get('base', '\nè¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n')
    format_template = format_section.get(case_format, '')
    
    system_prompt = base_prompt + "\n" + decomp_base + format_base + format_template
    
    # è°ƒç”¨æ¨¡å‹
    user_question = test_case["initial_question"]
    logger.info(f"ç”¨æˆ·é—®é¢˜: {user_question}")
    print(f"ç”¨æˆ·é—®é¢˜: {user_question}")
    
    response = client.chat_completion(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question}
        ]
    )
    
    model_response = response['choices'][0]['message']['content']
    logger.info(f"æ¨¡å‹å›å¤:\n{model_response}")
    
    # è¯¦ç»†è¾“å‡ºï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
    print_detail(f"\næ¨¡å‹å›å¤:\n{model_response}")
    
    # æå–ä»»åŠ¡
    extracted_tasks = extract_tasks_from_response(model_response, case_format)
    logger.info(f"æå–åˆ° {len(extracted_tasks)} ä¸ªä»»åŠ¡")
    
    print(f"\nâœ… æå–åˆ° {len(extracted_tasks)} ä¸ªå­ä»»åŠ¡")
    print_detail("\nå­ä»»åŠ¡åˆ—è¡¨:")
    for i, task in enumerate(extracted_tasks, 1):
        print_detail(f"  {i}. {task}")
    
    # éªŒè¯
    use_llm_similarity = config.get('evaluation.task_decomposition.use_llm_similarity', True)
    similarity_threshold = config.get('evaluation.task_decomposition.similarity_threshold', 0.7)
    
    ground_truth = test_case["stages"]["decomposition"]["ground_truth"]
    
    validation_result = validate_task_decomposition(
        model_response=model_response,
        ground_truth=ground_truth,
        mode="open",
        format_type=case_format,
        similarity_threshold=similarity_threshold,
        use_llm_similarity=use_llm_similarity
    )
    
    print(f"\néªŒè¯ç»“æœ:")
    print(f"  å¬å›ç‡: {validation_result['recall']:.2%}")
    print(f"  å‡†ç¡®ç‡: {validation_result['precision']:.2%}")
    print(f"  F1åˆ†æ•°: {validation_result['f1_score']:.2%}")
    
    min_recall = test_case["stages"]["decomposition"].get("min_recall", 0.6)
    min_precision = test_case["stages"]["decomposition"].get("min_precision", 0.5)
    
    passed = (validation_result['recall'] >= min_recall and 
              validation_result['precision'] >= min_precision)
    
    validation_result['passed'] = passed
    validation_result['model_response'] = model_response
    validation_result['extracted_tasks'] = extracted_tasks
    
    if passed:
        print(f"  âœ… é€šè¿‡ (å¬å›ç‡ >= {min_recall:.0%}, å‡†ç¡®ç‡ >= {min_precision:.0%})")
    else:
        print(f"  âŒ æœªé€šè¿‡ (è¦æ±‚: å¬å›ç‡ >= {min_recall:.0%}, å‡†ç¡®ç‡ >= {min_precision:.0%})")
    
    return validation_result, extracted_tasks


def run_planning_stage(
    test_case: Dict[str, Any],
    ground_truth_tasks: List[str],
    client: APIClient,
    config: Any
) -> Tuple[Dict[str, Any], List[List[str]]]:
    """
    è¿è¡Œä»»åŠ¡è§„åˆ’é˜¶æ®µ
    
    Args:
        ground_truth_tasks: ä»é˜¶æ®µ1çš„ground_truthå¾—åˆ°çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆä¸æ˜¯æ¨¡å‹è¾“å‡ºï¼ï¼‰
    
    Returns:
        (éªŒè¯ç»“æœ, è§„åˆ’çš„ä»»åŠ¡é¡ºåº)
    """
    logger = get_logger(__name__)
    logger.info("="*70)
    logger.info("é˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’")
    logger.info("="*70)
    
    print("\nğŸ”¹ é˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’")
    print("-" * 70)
    print(f"è¾“å…¥: é˜¶æ®µ1çš„ground_truthä»»åŠ¡åˆ—è¡¨ ({len(ground_truth_tasks)} ä¸ª)")
    print("æ³¨æ„: ä½¿ç”¨ground_truthè€Œä¸æ˜¯æ¨¡å‹è¾“å‡ºï¼Œä»¥ä¿è¯ä¸Šä¸‹æ–‡ç¨³å®šæ€§")
    
    # åŠ è½½æç¤ºè¯
    system_prompt_file = config.paths.prompts_dir / "system_prompt_2.json"
    prompt_data = read_json(system_prompt_file)
    
    base_prompt = prompt_data.get('base', '')
    task_planning = prompt_data.get('task', {}).get('task_planning', {})
    planning_base = task_planning.get('base', '')
    
    system_prompt = base_prompt + "\n" + planning_base
    
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼šä½¿ç”¨ground_truthä»»åŠ¡åˆ—è¡¨
    user_message = "å·²æ‹†è§£å¥½çš„å­ä»»åŠ¡åˆ—è¡¨ï¼š\n" + "\n".join([f"- {task}" for task in ground_truth_tasks])
    
    logger.info(f"ç”¨æˆ·æ¶ˆæ¯:\n{user_message}")
    logger.info(f"è¾“å…¥æ¥æº: ground_truth (é˜¶æ®µ1)")
    
    # è°ƒç”¨æ¨¡å‹
    response = client.chat_completion(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    )
    
    model_response = response['choices'][0]['message']['content']
    logger.info(f"æ¨¡å‹å›å¤:\n{model_response}")
    
    print(f"\nâœ… è§„åˆ’å®Œæˆ")
    print_detail(f"\næ¨¡å‹è§„åˆ’ç»“æœ:")
    print_detail(model_response)
    
    # éªŒè¯ï¼ˆä½¿ç”¨ground_truthä»»åŠ¡åˆ—è¡¨ï¼‰
    dependencies = test_case["stages"]["planning"]["dependencies"]
    
    validation_result = validate_task_planning(
        model_response=model_response,
        ground_truth_tasks=ground_truth_tasks,  # ä½¿ç”¨ground_truthä»»åŠ¡åˆ—è¡¨
        dependencies=dependencies
    )
    
    print(f"\néªŒè¯ç»“æœ:")
    print(f"  è¦†ç›–åº¦: {validation_result['coverage']:.2%}")
    print(f"  é¡ºåºæ­£ç¡®ç‡: {validation_result['order_correctness']:.2%}")
    print(f"  å±‚çº§æ•ˆç‡: {validation_result['level_efficiency']:.2%}")
    print(f"  ç»¼åˆå¾—åˆ†: {validation_result['overall_score']:.2%}")
    
    min_coverage = test_case["stages"]["planning"].get("min_coverage", 0.7)
    min_order = test_case["stages"]["planning"].get("min_order_correctness", 0.8)
    
    passed = (validation_result['coverage'] >= min_coverage and 
              validation_result['order_correctness'] >= min_order)
    
    validation_result['passed'] = passed
    validation_result['model_response'] = model_response
    validation_result['input_tasks'] = ground_truth_tasks
    validation_result['input_source'] = "ground_truth"
    
    if passed:
        print(f"  âœ… é€šè¿‡")
    else:
        print(f"  âŒ æœªé€šè¿‡")
    
    # æå–è§„åˆ’çš„ä»»åŠ¡é¡ºåº
    planned_order = validation_result.get('model_plan', [])
    
    return validation_result, planned_order


def run_execution_stage(
    test_case: Dict[str, Any],
    ground_truth_plan: List[List[str]],
    config: Any
) -> Dict[str, Any]:
    """
    è¿è¡Œä»»åŠ¡æ‰§è¡Œé˜¶æ®µ
    
    Args:
        ground_truth_plan: ä»é˜¶æ®µ2çš„ground_truthå¾—åˆ°çš„ä»»åŠ¡æ‰§è¡Œè®¡åˆ’ï¼ˆä¸æ˜¯æ¨¡å‹è¾“å‡ºï¼ï¼‰
    
    Returns:
        æ‰§è¡Œç»“æœ
    """
    logger = get_logger(__name__)
    logger.info("="*70)
    logger.info("é˜¶æ®µ3ï¼šä»»åŠ¡æ‰§è¡Œ")
    logger.info("="*70)
    
    print("\nğŸ”¹ é˜¶æ®µ3ï¼šä»»åŠ¡æ‰§è¡Œ")
    print("-" * 70)
    print(f"è¾“å…¥: é˜¶æ®µ2çš„ground_truthæ‰§è¡Œè®¡åˆ’")
    print("æ³¨æ„: ä½¿ç”¨ground_truth planè€Œä¸æ˜¯æ¨¡å‹è¾“å‡ºï¼Œä»¥ä¿è¯ä¸Šä¸‹æ–‡ç¨³å®šæ€§")
    
    task_data = test_case["task_data"]
    
    print(f"\nä»»åŠ¡ç±»å‹: {task_data['tag']}")
    print(f"\næ‰§è¡Œè®¡åˆ’:")
    for i, level in enumerate(ground_truth_plan, 1):
        print(f"  å±‚çº§{i}: {level}")
    
    logger.info(f"è¾“å…¥è®¡åˆ’: {ground_truth_plan}")
    logger.info(f"è¾“å…¥æ¥æº: ground_truth (é˜¶æ®µ2)")
    
    # æ„å»ºquestionå¯¹è±¡ï¼ˆä»task_dataï¼‰
    question = {
        "tag": task_data["tag"],
        "number": task_data["number"],
        "question": test_case["initial_question"],
        "answer": "æˆ‘ä¼šæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ã€‚",  # æ·»åŠ answerå­—æ®µ
        "plan_answer": str(ground_truth_plan)  # æ·»åŠ plan_answerå­—æ®µ
    }
    
    # æ·»åŠ é¢å¤–çš„éªŒè¯å­—æ®µ
    if "test_case" in task_data:
        question["test_case"] = task_data["test_case"]
    if "names" in task_data:
        question["names"] = task_data["names"]
    if "function" in task_data:
        question["function"] = task_data["function"]
    if "sums" in task_data:
        question["sums"] = task_data["sums"]
    
    # ä½¿ç”¨EvaluationEngineè¿è¡Œå•ä¸ªä»»åŠ¡
    engine = EvaluationEngine(
        model=config.api.default_model,
        use_stream=config.api.stream_enabled
    )
    
    # åŠ è½½ç³»ç»Ÿæç¤ºè¯å’Œå·¥å…·
    from lib.core.utils import read_json
    system_prompt_file = config.paths.prompts_dir / "system_prompt_2.json"
    tool_list_file = config.paths.prompts_dir / "tool_list.json"
    
    system_prompt_data = read_json(system_prompt_file)
    tools = read_json(tool_list_file)
    
    # æ„å»ºä»»åŠ¡æ‰§è¡Œçš„ç³»ç»Ÿæç¤ºè¯
    system_prompt = system_prompt_data['base']  # åŸºç¡€æç¤ºè¯
    system_prompt += "\n" + system_prompt_data['task']['task_exe']['base']  # ä»»åŠ¡æ‰§è¡Œæç¤ºè¯
    
    # å‡†å¤‡ground_truthï¼ˆä½¿ç”¨ä»»åŠ¡æ‰§è¡Œè®¡åˆ’ï¼‰
    ground_truth = {
        "question": test_case["initial_question"],  # æ·»åŠ questionå­—æ®µ
        "answer": "æˆ‘ä¼šæŒ‰ç…§ç»™å®šçš„ä»»åŠ¡åˆ—è¡¨æ¥æ‰§è¡Œã€‚",  # æ¨¡æ‹Ÿçš„answer
        "SubTasks": test_case["stages"]["decomposition"]["ground_truth"],
        "plan_answer": str(ground_truth_plan)
    }
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
    import time
    output_dir = config.paths.outputs_dir / f"exec_{int(time.time())}"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"result_{task_data['number']}.json"
    
    # è¿è¡Œå•ä¸ªä»»åŠ¡
    print("\nå¼€å§‹æ‰§è¡Œä»»åŠ¡...")
    try:
        result = engine.run_single_task(
            question=question,
            ground_truth=ground_truth,
            system_prompt=system_prompt,  # ä½¿ç”¨æ„å»ºå¥½çš„system_prompt
            tools=tools,
            output_file=output_file
        )
        
        passed = result.get('pass', False)
        
        print(f"\næ‰§è¡Œç»“æœ:")
        print(f"  ä»»åŠ¡: {question['tag']}_{question['number']}")
        print(f"  é€šè¿‡: {'âœ“' if passed else 'âœ—'}")
        print(f"  è½®æ¬¡: {result.get('metrics', {}).get('total_rounds', 0)}")
        print(f"  å·¥å…·è°ƒç”¨: {result.get('metrics', {}).get('tool_calls', 0)}")
        
        if passed:
            print(f"  âœ… é€šè¿‡")
        else:
            print(f"  âŒ æœªé€šè¿‡")
            if 'error' in result:
                print(f"  é”™è¯¯: {result['error']}")
        
        return {
            'passed': passed,
            'task_result': result,
            'metrics': result.get('metrics', {}),
            'input_plan': ground_truth_plan,
            'input_source': 'ground_truth',
            'output_file': str(output_file)
        }
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        return {
            'passed': False,
            'error': str(e),
            'input_plan': ground_truth_plan,
            'input_source': 'ground_truth'
        }


def run_batch_evaluation(
    test_cases: List[Dict[str, Any]],
    client: APIClient,
    config: Any
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šæŒ‰é˜¶æ®µæ‰¹é‡å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    
    ä¼˜ç‚¹ï¼š
    - æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ï¼ˆå¯ä»¥ä¸€æ¬¡æ€§çœ‹åˆ°æ‰€æœ‰é˜¶æ®µ1çš„ç»“æœï¼‰
    - ä¾¿äºè°ƒè¯•å’Œåˆ†æï¼ˆåŒä¸€é˜¶æ®µçš„ç»“æœé›†ä¸­æ˜¾ç¤ºï¼‰
    
    é€»è¾‘ï¼š
    - é˜¶æ®µ1ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„åˆ†è§£é˜¶æ®µ
    - é˜¶æ®µ2ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„è§„åˆ’é˜¶æ®µï¼ˆä½¿ç”¨é˜¶æ®µ1çš„ground_truthï¼‰
    - é˜¶æ®µ3ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œé˜¶æ®µï¼ˆä½¿ç”¨é˜¶æ®µ2çš„ground_truthï¼‰
    
    Args:
        test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        client: APIå®¢æˆ·ç«¯
        config: é…ç½®å¯¹è±¡
    
    Returns:
        è¯„æµ‹ç»“æœåˆ—è¡¨
    """
    logger = get_logger(__name__)
    all_results = []
    
    # åˆå§‹åŒ–æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœç»“æ„
    for test_case in test_cases:
        all_results.append({
            "test_case_id": test_case['id'],
            "test_case_name": test_case['name'],
            "stages": {}
        })
    
    print(f"\nğŸ“¦ æ‰¹é‡å¤„ç†æ¨¡å¼: å°†æŒ‰é˜¶æ®µæ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"  ä¼˜åŠ¿: æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ï¼Œä¾¿äºå¯¹æ¯”åŒä¸€é˜¶æ®µçš„æ‰€æœ‰ç»“æœ\n")
    
    # ========== é˜¶æ®µ1ï¼šæ‰¹é‡å¤„ç†ä»»åŠ¡åˆ†è§£ ==========
    print(f"\n{'='*70}")
    print(f"é˜¶æ®µ 1/3: ä»»åŠ¡åˆ†è§£ - æ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"{'='*70}\n")
    
    decomp_results = []
    ground_truth_tasks_list = []
    
    for i, test_case in enumerate(test_cases):
        print(f"[{i+1}/{len(test_cases)}] {test_case['name']}")
        
        try:
            decomp_result, extracted_tasks = run_decomposition_stage(test_case, client, config)
            decomp_results.append(decomp_result)
            
            # è·å–ground_truthä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºä¼ é€’ç»™é˜¶æ®µ2ï¼‰
            ground_truth_tasks = test_case["stages"]["decomposition"]["ground_truth"]
            ground_truth_tasks_list.append(ground_truth_tasks)
            
            all_results[i]["stages"]["decomposition"] = decomp_result
            
            print(f"  æ¨¡å‹è¾“å‡º: {len(extracted_tasks)} ä¸ªä»»åŠ¡")
            print(f"  Ground Truth: {len(ground_truth_tasks)} ä¸ªä»»åŠ¡")
            print(f"  è¯„æµ‹ç»“æœ: {'âœ… é€šè¿‡' if decomp_result['passed'] else 'âŒ æœªé€šè¿‡'}")
            print(f"  F1åˆ†æ•°: {decomp_result['f1_score']:.2%}\n")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç”¨ä¾‹ {test_case['name']} åˆ†è§£é˜¶æ®µå¤±è´¥: {e}", exc_info=True)
            print(f"  âŒ å¤±è´¥: {e}\n")
            decomp_results.append({"error": str(e), "passed": False})
            ground_truth_tasks_list.append([])
    
    print(f"\nğŸ“Š é˜¶æ®µ1æ±‡æ€»:")
    passed_count = sum(1 for r in decomp_results if r.get('passed', False))
    print(f"  é€šè¿‡ç‡: {passed_count}/{len(test_cases)} ({passed_count/len(test_cases)*100:.1f}%)")
    
    # ========== é˜¶æ®µ2ï¼šæ‰¹é‡å¤„ç†ä»»åŠ¡è§„åˆ’ ==========
    print(f"\n{'='*70}")
    print(f"é˜¶æ®µ 2/3: ä»»åŠ¡è§„åˆ’ - æ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"{'='*70}\n")
    
    planning_results = []
    ground_truth_plans_list = []
    
    for i, test_case in enumerate(test_cases):
        print(f"[{i+1}/{len(test_cases)}] {test_case['name']}")
        
        # ä½¿ç”¨é˜¶æ®µ1çš„ground_truthä½œä¸ºè¾“å…¥
        ground_truth_tasks = ground_truth_tasks_list[i]
        
        if not ground_truth_tasks:
            print(f"  âš ï¸  è·³è¿‡ï¼ˆé˜¶æ®µ1å¤±è´¥ï¼‰\n")
            planning_results.append({"error": "é˜¶æ®µ1å¤±è´¥", "passed": False})
            ground_truth_plans_list.append([])
            continue
        
        try:
            planning_result, planned_order = run_planning_stage(
                test_case,
                ground_truth_tasks,  # â† ä½¿ç”¨ground_truth
                client,
                config
            )
            planning_results.append(planning_result)
            
            # è·å–ground_truthè®¡åˆ’ï¼ˆç”¨äºä¼ é€’ç»™é˜¶æ®µ3ï¼‰
            if "ground_truth_plan" in test_case["stages"]["planning"]:
                ground_truth_plan = test_case["stages"]["planning"]["ground_truth_plan"]
            else:
                ground_truth_plan = create_default_plan_from_dependencies(
                    ground_truth_tasks,
                    test_case["stages"]["planning"]["dependencies"]
                )
            ground_truth_plans_list.append(ground_truth_plan)
            
            all_results[i]["stages"]["planning"] = planning_result
            
            print(f"  è¾“å…¥: ground_truth ({len(ground_truth_tasks)} ä¸ªä»»åŠ¡)")
            print(f"  æ¨¡å‹è¾“å‡º: {len(planned_order)} å±‚è®¡åˆ’")
            print(f"  Ground Truth: {len(ground_truth_plan)} å±‚è®¡åˆ’")
            print(f"  è¯„æµ‹ç»“æœ: {'âœ… é€šè¿‡' if planning_result['passed'] else 'âŒ æœªé€šè¿‡'}")
            print(f"  ç»¼åˆå¾—åˆ†: {planning_result['overall_score']:.2%}\n")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç”¨ä¾‹ {test_case['name']} è§„åˆ’é˜¶æ®µå¤±è´¥: {e}", exc_info=True)
            print(f"  âŒ å¤±è´¥: {e}\n")
            planning_results.append({"error": str(e), "passed": False})
            ground_truth_plans_list.append([])
    
    print(f"\nğŸ“Š é˜¶æ®µ2æ±‡æ€»:")
    passed_count = sum(1 for r in planning_results if r.get('passed', False))
    print(f"  é€šè¿‡ç‡: {passed_count}/{len(test_cases)} ({passed_count/len(test_cases)*100:.1f}%)")
    
    # ========== é˜¶æ®µ3ï¼šæ‰¹é‡å¤„ç†ä»»åŠ¡æ‰§è¡Œ ==========
    print(f"\n{'='*70}")
    print(f"é˜¶æ®µ 3/3: ä»»åŠ¡æ‰§è¡Œ - æ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"{'='*70}\n")
    
    execution_results = []
    
    for i, test_case in enumerate(test_cases):
        print(f"[{i+1}/{len(test_cases)}] {test_case['name']}")
        
        # ä½¿ç”¨é˜¶æ®µ2çš„ground_truthä½œä¸ºè¾“å…¥
        ground_truth_plan = ground_truth_plans_list[i]
        
        if not ground_truth_plan:
            print(f"  âš ï¸  è·³è¿‡ï¼ˆé˜¶æ®µ2å¤±è´¥ï¼‰\n")
            execution_results.append({"error": "é˜¶æ®µ2å¤±è´¥", "passed": False})
            continue
        
        try:
            execution_result = run_execution_stage(
                test_case,
                ground_truth_plan,  # â† ä½¿ç”¨ground_truth
                config
            )
            execution_results.append(execution_result)
            
            all_results[i]["stages"]["execution"] = execution_result
            
            print(f"  è¾“å…¥: ground_truth ({len(ground_truth_plan)} å±‚è®¡åˆ’)")
            print(f"  è¯„æµ‹ç»“æœ: {'âœ… é€šè¿‡' if execution_result['passed'] else 'âŒ æœªé€šè¿‡'}\n")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç”¨ä¾‹ {test_case['name']} æ‰§è¡Œé˜¶æ®µå¤±è´¥: {e}", exc_info=True)
            print(f"  âŒ å¤±è´¥: {e}\n")
            execution_results.append({"error": str(e), "passed": False})
    
    print(f"\nğŸ“Š é˜¶æ®µ3æ±‡æ€»:")
    passed_count = sum(1 for r in execution_results if r.get('passed', False))
    print(f"  é€šè¿‡ç‡: {passed_count}/{len(test_cases)} ({passed_count/len(test_cases)*100:.1f}%)")
    
    # ========== æ•´ä½“æ±‡æ€» ==========
    print(f"\n{'='*70}")
    print(f"æ‰¹é‡å¤„ç†å®Œæˆ - æ•´ä½“æ±‡æ€»")
    print(f"{'='*70}\n")
    
    for i, result in enumerate(all_results):
        decomp_passed = result["stages"].get("decomposition", {}).get("passed", False)
        planning_passed = result["stages"].get("planning", {}).get("passed", False)
        execution_passed = result["stages"].get("execution", {}).get("passed", False)
        
        all_passed = decomp_passed and planning_passed and execution_passed
        
        result["overall"] = {
            "all_stages_passed": all_passed,
            "summary": f"åˆ†è§£: {'âœ…' if decomp_passed else 'âŒ'} | "
                      f"è§„åˆ’: {'âœ…' if planning_passed else 'âŒ'} | "
                      f"æ‰§è¡Œ: {'âœ…' if execution_passed else 'âŒ'}"
        }
        
        print(f"[{i+1}] {result['test_case_name']}")
        print(f"    {result['overall']['summary']}")
    
    total_passed = sum(1 for r in all_results if r["overall"]["all_stages_passed"])
    print(f"\nğŸ“ˆ æ€»ä½“é€šè¿‡ç‡: {total_passed}/{len(test_cases)} ({total_passed/len(test_cases)*100:.1f}%)")
    
    return all_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸‰é˜¶æ®µè¿ç»­è¯„æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--test-file",
        default="data/three_stage_test_cases.json",
        help="æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--test-id",
        help="æŒ‡å®šæµ‹è¯•ç”¨ä¾‹ID"
    )
    
    parser.add_argument(
        "--model",
        help="æŒ‡å®šæ¨¡å‹åç§°"
    )
    
    parser.add_argument(
        "--output",
        help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--batch",
        action="store_true",
        help="æ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„åŒä¸€é˜¶æ®µï¼Œé€Ÿåº¦æ›´å¿«ï¼‰"
    )
    
    parser.add_argument(
        "--show-details",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ¨¡å‹è¾“å‡ºã€ä»£ç å†…å®¹ç­‰ï¼Œé»˜è®¤ä¸æ˜¾ç¤ºï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºæ§åˆ¶
    set_show_details(args.show_details)
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print_banner()
    
    if not args.show_details:
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --show-details æŸ¥çœ‹æ¨¡å‹è¾“å‡ºå’Œä»£ç å†…å®¹")
        print("   æ‰€æœ‰è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ä¸­\n")
    
    # åŠ è½½é…ç½®
    try:
        config = get_config()
        LoggerManager.initialize(config.paths.logs_dir)
        logger = get_logger(__name__)
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    test_file = Path(args.test_file)
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return 1
    
    with open(test_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    if args.test_id:
        test_cases = [tc for tc in test_cases if tc['id'] == args.test_id]
        if not test_cases:
            print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹: {args.test_id}")
            return 1
    
    # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
    model = args.model or config.api.default_model
    client = APIClient(model=model)
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
    if args.batch:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print("\nğŸš€ ä½¿ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼")
        print("  ç‰¹ç‚¹: æŒ‰é˜¶æ®µæ‰¹é‡å¤„ç†ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä¾¿äºå¯¹æ¯”åŒä¸€é˜¶æ®µçš„æ‰€æœ‰ç»“æœ\n")
        all_results = run_batch_evaluation(test_cases, client, config)
    else:
        # é€ä¸ªå¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        print("\nğŸ”„ ä½¿ç”¨é€ä¸ªå¤„ç†æ¨¡å¼")
        print("  ç‰¹ç‚¹: æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¿ç»­å®Œæˆä¸‰ä¸ªé˜¶æ®µï¼Œä¾¿äºè·Ÿè¸ªå•ä¸ªç”¨ä¾‹çš„å®Œæ•´æµç¨‹\n")
        all_results = []
        
        for test_case in test_cases:
            print(f"\n{'='*70}")
            print(f"æµ‹è¯•ç”¨ä¾‹: {test_case['name']}")
            print(f"æè¿°: {test_case['description']}")
            print(f"{'='*70}")
            
            result = {
                "test_case_id": test_case['id'],
                "test_case_name": test_case['name'],
                "stages": {}
            }
            
            try:
                # é˜¶æ®µ1ï¼šä»»åŠ¡åˆ†è§£
                decomp_result, extracted_tasks = run_decomposition_stage(test_case, client, config)
                result["stages"]["decomposition"] = decomp_result
                
                # è·å–ground_truthä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºä¼ é€’ç»™é˜¶æ®µ2ï¼‰
                ground_truth_tasks = test_case["stages"]["decomposition"]["ground_truth"]
                
                print(f"\nğŸ“‹ ä¸Šä¸‹æ–‡ä¼ é€’ï¼š")
                print(f"  é˜¶æ®µ1æ¨¡å‹è¾“å‡º: {len(extracted_tasks)} ä¸ªä»»åŠ¡ â†’ ä»…ç”¨äºè¯„æµ‹")
                print(f"  ä¼ é€’ç»™é˜¶æ®µ2: ground_truth ({len(ground_truth_tasks)} ä¸ªä»»åŠ¡)")
                logger.info(f"é˜¶æ®µ1å®Œæˆï¼Œä¼ é€’ground_truthç»™é˜¶æ®µ2: {ground_truth_tasks}")
                
                # é˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’ï¼ˆä½¿ç”¨ground_truthä»»åŠ¡åˆ—è¡¨ï¼‰
                planning_result, planned_order = run_planning_stage(
                    test_case, 
                    ground_truth_tasks,  # â† ä¼ é€’ground_truthï¼Œä¸æ˜¯extracted_tasksï¼
                    client, 
                    config
                )
                result["stages"]["planning"] = planning_result
                
                # è·å–ground_truthè®¡åˆ’ï¼ˆç”¨äºä¼ é€’ç»™é˜¶æ®µ3ï¼‰
                # å¦‚æœæµ‹è¯•ç”¨ä¾‹ä¸­æœ‰ground_truth_planï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™åˆ›å»ºé»˜è®¤è®¡åˆ’
                if "ground_truth_plan" in test_case["stages"]["planning"]:
                    ground_truth_plan = test_case["stages"]["planning"]["ground_truth_plan"]
                else:
                    # æ ¹æ®ä¾èµ–å…³ç³»åˆ›å»ºé»˜è®¤çš„ground_truthè®¡åˆ’
                    ground_truth_plan = create_default_plan_from_dependencies(
                        ground_truth_tasks,
                        test_case["stages"]["planning"]["dependencies"]
                    )
                
                print(f"\nğŸ“‹ ä¸Šä¸‹æ–‡ä¼ é€’ï¼š")
                print(f"  é˜¶æ®µ2æ¨¡å‹è¾“å‡º: {len(planned_order)} å±‚è®¡åˆ’ â†’ ä»…ç”¨äºè¯„æµ‹")
                print(f"  ä¼ é€’ç»™é˜¶æ®µ3: ground_truth plan ({len(ground_truth_plan)} å±‚)")
                logger.info(f"é˜¶æ®µ2å®Œæˆï¼Œä¼ é€’ground_truth planç»™é˜¶æ®µ3: {ground_truth_plan}")
                
                # é˜¶æ®µ3ï¼šä»»åŠ¡æ‰§è¡Œï¼ˆä½¿ç”¨ground_truthè®¡åˆ’ï¼‰
                execution_result = run_execution_stage(
                    test_case, 
                    ground_truth_plan,  # â† ä¼ é€’ground_truth planï¼Œä¸æ˜¯planned_orderï¼
                    config
                )
                result["stages"]["execution"] = execution_result
                
                # æ•´ä½“è¯„ä»·
                result["overall"] = {
                    "all_stages_passed": all([
                        decomp_result['passed'],
                        planning_result['passed'],
                        execution_result['passed']
                    ]),
                    "summary": f"åˆ†è§£: {'âœ…' if decomp_result['passed'] else 'âŒ'} | "
                              f"è§„åˆ’: {'âœ…' if planning_result['passed'] else 'âŒ'} | "
                              f"æ‰§è¡Œ: {'âœ…' if execution_result['passed'] else 'âŒ'}"
                }
                
                print(f"\n{'='*70}")
                print(f"æ•´ä½“ç»“æœ: {result['overall']['summary']}")
                print(f"{'='*70}")
                
            except Exception as e:
                logger.error(f"è¯„æµ‹å¤±è´¥: {e}", exc_info=True)
                print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
                result["error"] = str(e)
            
            all_results.append(result)
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "="*70)
    print("ğŸ“Š ä¸‰é˜¶æ®µè¯„æµ‹æ±‡æ€»ç»Ÿè®¡")
    print("="*70)
    
    total_cases = len(all_results)
    
    # ç»Ÿè®¡å„é˜¶æ®µ
    decomp_passed = sum(1 for r in all_results if r.get("stages", {}).get("decomposition", {}).get("passed", False))
    planning_passed = sum(1 for r in all_results if r.get("stages", {}).get("planning", {}).get("passed", False))
    execution_passed = sum(1 for r in all_results if r.get("stages", {}).get("execution", {}).get("passed", False))
    all_stages_passed = sum(1 for r in all_results if r.get("overall", {}).get("all_stages_passed", False))
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    decomp_scores = [r.get("stages", {}).get("decomposition", {}).get("metrics", {}).get("overall_score", 0) 
                     for r in all_results if "stages" in r and "decomposition" in r["stages"]]
    planning_scores = [r.get("stages", {}).get("planning", {}).get("metrics", {}).get("overall_score", 0) 
                       for r in all_results if "stages" in r and "planning" in r["stages"]]
    
    avg_decomp = sum(decomp_scores) / len(decomp_scores) if decomp_scores else 0
    avg_planning = sum(planning_scores) / len(planning_scores) if planning_scores else 0
    
    print(f"\næ€»æµ‹è¯•ç”¨ä¾‹æ•°: {total_cases}")
    print(f"\nå„é˜¶æ®µé€šè¿‡æƒ…å†µ:")
    print(f"  é˜¶æ®µ1 (ä»»åŠ¡åˆ†è§£): {decomp_passed}/{total_cases} é€šè¿‡ ({decomp_passed/total_cases*100:.1f}%)")
    print(f"    - å¹³å‡ç»¼åˆå¾—åˆ†: {avg_decomp:.2%}")
    print(f"  é˜¶æ®µ2 (ä»»åŠ¡è§„åˆ’): {planning_passed}/{total_cases} é€šè¿‡ ({planning_passed/total_cases*100:.1f}%)")
    print(f"    - å¹³å‡ç»¼åˆå¾—åˆ†: {avg_planning:.2%}")
    print(f"  é˜¶æ®µ3 (ä»»åŠ¡æ‰§è¡Œ): {execution_passed}/{total_cases} é€šè¿‡ ({execution_passed/total_cases*100:.1f}%)")
    print(f"\nå®Œæ•´æµç¨‹é€šè¿‡: {all_stages_passed}/{total_cases} ({all_stages_passed/total_cases*100:.1f}%)")
    
    # è¯¦ç»†ç»“æœåˆ—è¡¨
    print(f"\nè¯¦ç»†ç»“æœ:")
    for i, r in enumerate(all_results, 1):
        case_name = r.get("test_case_name", f"Test {i}")
        overall = r.get("overall", {})
        summary = overall.get("summary", "N/A")
        print(f"  {i}. {case_name}")
        print(f"     {summary}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_file = Path(args.output)
    else:
        import time
        timestamp = int(time.time())
        output_file = config.paths.outputs_dir / f"three_stage_{timestamp}.json"
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    LoggerManager.shutdown()
    return 0


if __name__ == "__main__":
    sys.exit(main())

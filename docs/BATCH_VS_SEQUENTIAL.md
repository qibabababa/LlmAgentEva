# 三阶段连续评测：批量处理 vs 逐个处理

## 概述

三阶段连续评测系统提供了两种处理模式，以满足不同的使用场景。

**测试用例总数**: 6个测试用例，覆盖所有任务类型
- test_001: Bug修复任务 (fix_bug)
- test_002: 代码转换任务 (convert)
- test_003: 代码重构任务 (refactor)
- test_004: 环境配置任务 (env)
- test_005: 文档生成任务 (sum)
- test_006: 函数拆分任务 (split)

## 两种模式对比

### 1. 逐个处理模式（默认）

**命令**：
```bash
python bin/run_three_stage_continuous.py  # 运行所有6个测试用例
python bin/run_three_stage_continuous.py --test-id test_001  # 运行单个用例
# 或
python run.py three-stage
```

**执行流程**：
```
测试用例1 (fix_bug):
  ├── 阶段1: 任务分解 → 评测模型输出
  │   └── 传递 ground_truth 到阶段2
  ├── 阶段2: 任务规划 → 评测模型输出
  │   └── 传递 ground_truth 到阶段3
  └── 阶段3: 任务执行 → 评测模型输出

测试用例2 (convert):
  ├── 阶段1: 任务分解 → 评测模型输出
  │   └── 传递 ground_truth 到阶段2
  ├── 阶段2: 任务规划 → 评测模型输出
  │   └── 传递 ground_truth 到阶段3
  └── 阶段3: 任务执行 → 评测模型输出

... (共6个测试用例)
```

**特点**：
- ✅ 每个测试用例**连续完成三个阶段**
- ✅ 实时看到每个用例的**完整端到端结果**
- ✅ 便于**追踪单个用例**的详细流程
- ✅ 上下文信息连续清晰

**适用场景**：
- 调试和分析单个测试用例
- 需要实时观察每个用例的完整流程
- 测试用例数量较少（1-5个）
- 深入分析模型在完整流程中的表现

**输出示例**：
```
======================================================================
[1/6] Bug修复任务
======================================================================

阶段 1: 任务分解
用户问题: 当前目录下有一个Python文件 bug_code_1.py...
...
  召回率: 85.71%
  准确率: 85.71%
  F1分数: 85.71%
  ✅ 通过

📋 上下文传递：
  阶段1模型输出: 6 个任务 → 仅用于评测
  传递给阶段2: ground_truth (7 个任务)

阶段 2: 任务规划
...
  ✅ 通过

📋 上下文传递：
  阶段2模型输出: 4 层计划 → 仅用于评测
  传递给阶段3: ground_truth plan (4 层)

阶段 3: 任务执行
...
  ✅ 通过

======================================================================
整体结果: 分解: ✅ | 规划: ✅ | 执行: ✅
======================================================================

[继续下一个测试用例...]
```

---

### 2. 批量处理模式

**命令**：
```bash
python bin/run_three_stage_continuous.py --batch  # 批量处理所有6个测试用例
python bin/run_three_stage_continuous.py --batch --test-id test_001  # 批量模式运行单个用例
# 或
python run.py three-stage --batch
```

**执行流程**：
```
阶段1: 任务分解（批量处理所有6个测试用例）
  ├── 测试用例1 (fix_bug): 分解 → 评测
  ├── 测试用例2 (convert): 分解 → 评测
  ├── 测试用例3 (refactor): 分解 → 评测
  ├── 测试用例4 (env): 分解 → 评测
  ├── 测试用例5 (sum): 分解 → 评测
  ├── 测试用例6 (split): 分解 → 评测
  └── 汇总阶段1结果

阶段2: 任务规划（批量处理所有6个测试用例，使用阶段1的ground_truth）
  ├── 测试用例1: 规划 → 评测
  ├── 测试用例2: 规划 → 评测
  ├── 测试用例3: 规划 → 评测
  ├── 测试用例4: 规划 → 评测
  ├── 测试用例5: 规划 → 评测
  ├── 测试用例6: 规划 → 评测
  └── 汇总阶段2结果

阶段3: 任务执行（批量处理所有6个测试用例，使用阶段2的ground_truth）
  ├── 测试用例1: 执行 → 评测
  ├── 测试用例2: 执行 → 评测
  ├── 测试用例3: 执行 → 评测
  ├── 测试用例4: 执行 → 评测
  ├── 测试用例5: 执行 → 评测
  ├── 测试用例6: 执行 → 评测
  └── 汇总阶段3结果

整体汇总（6个测试用例的统计）
```

**特点**：
- ✅ **按阶段批量处理**，执行速度更快
- ✅ 便于**横向对比**同一阶段的所有结果
- ✅ 可以一次性看到所有6个用例在某个阶段的表现
- ✅ 更高效的执行顺序

**适用场景**：
- 快速评测大量测试用例（所有6个用例）
- 需要对比分析同一阶段的性能
- 批量实验和性能测试
- 关注整体统计数据而非单个用例的详细流程

**输出示例**：
```
======================================================================
阶段 1/3: 任务分解 - 批量处理 6 个测试用例
======================================================================

[1/6] Bug修复任务
  模型输出: 5 个任务
  Ground Truth: 5 个任务
  评测结果: ✅ 通过
  F1分数: 85.71%

[2/6] 代码转换任务
  模型输出: 5 个任务
  Ground Truth: 5 个任务
  评测结果: ✅ 通过
  F1分数: 83.33%

[3/6] 代码重构任务
  模型输出: 6 个任务
  Ground Truth: 7 个任务
  评测结果: ❌ 未通过
  F1分数: 63.16%

📊 阶段1汇总:
  通过率: 2/3 (66.7%)

======================================================================
阶段 2/3: 任务规划 - 批量处理 3 个测试用例
======================================================================

[1/6] Bug修复任务
  输入: ground_truth (5 个任务)
  模型输出: 4 层计划
  Ground Truth: 4 层计划
  评测结果: ✅ 通过
  综合得分: 95.00%

[继续处理其他5个用例...]

📊 阶段2汇总:
  通过率: 4/6 (66.7%)

======================================================================
阶段 3/3: 任务执行 - 批量处理 6 个测试用例
======================================================================

[继续处理...]

📊 阶段3汇总:
  通过率: 3/6 (50.0%)

======================================================================
批量处理完成 - 整体汇总
======================================================================

总测试用例数: 6

各阶段通过情况:
  阶段1 (任务分解): 5/6 通过 (83.3%)
    - 平均综合得分: 75.50%
  阶段2 (任务规划): 4/6 通过 (66.7%)
    - 平均综合得分: 68.20%
  阶段3 (任务执行): 3/6 通过 (50.0%)

完整流程通过: 3/6 (50.0%)

详细结果:
  1. Bug修复任务
     分解: ✅ | 规划: ✅ | 执行: ✅
  2. 代码转换任务
     分解: ✅ | 规划: ✅ | 执行: ❌
  3. 代码重构任务
     分解: ✅ | 规划: ✅ | 执行: ❌
  4. 环境配置任务
     分解: ✅ | 规划: ❌ | 执行: ❌
  5. 文档生成任务
     分解: ✅ | 规划: ❌ | 执行: ❌
  6. 函数拆分任务
     分解: ❌ | 规划: ❌ | 执行: ❌
```

---

## 核心原则（两种模式相同）

**无论哪种模式，核心逻辑完全一致**：

1. **每个测试用例都完整经过三个阶段**
2. **评测模型输出**：每个阶段都评测模型的实际表现
3. **传递ground_truth**：下一阶段使用标准答案作为输入
4. **上下文稳定**：避免错误信息的连锁传播

```
阶段1（分解）:
  模型输出 → 评测（计算F1分数）
  ground_truth → 传递给阶段2 ✓

阶段2（规划）:
  输入: ground_truth任务列表
  模型输出 → 评测（计算覆盖度、顺序正确性）
  ground_truth plan → 传递给阶段3 ✓

阶段3（执行）:
  输入: ground_truth计划
  模型输出 → 评测（验证最终结果）
```

## 性能对比

假设有10个测试用例，每个阶段平均耗时1分钟：

| 模式 | 总耗时 | 实时反馈 | 便于对比 |
|------|--------|----------|----------|
| 逐个处理 | ~30分钟 | ✅ 实时 | ❌ 分散 |
| 批量处理 | ~30分钟 | ⚠️  按阶段 | ✅ 集中 |

**注意**：实际执行时间相同，但批量模式更便于分析和对比。

## 选择建议

### 选择逐个处理模式，当你：
- 🔍 需要调试和分析单个测试用例
- 📝 想要详细的端到端流程日志
- 🎯 测试用例数量较少（1-5个）
- 🔬 进行深入的模型行为分析

### 选择批量处理模式，当你：
- ⚡ 需要快速评测大量用例（5个以上）
- 📊 关注整体统计和横向对比
- 🧪 进行批量实验和性能测试
- 📈 需要生成汇总报告

## 使用示例

### 示例1：调试单个用例
```bash
# 使用逐个处理模式
python run.py three-stage --test-id test_001
```

### 示例2：快速评测所有用例
```bash
# 使用批量处理模式
python run.py three-stage --batch
```

### 示例3：评测特定用例集合
```bash
# 可以结合test-id和batch使用
python run.py three-stage --batch --test-id test_001
# 注意：单个用例使用batch模式没有太大意义
```

## 总结

两种模式**逻辑完全一致**，只是**展示顺序不同**：

- **逐个处理**：先完整展示用例1的三个阶段，再展示用例2...
- **批量处理**：先展示所有用例的阶段1，再展示所有用例的阶段2...

选择哪种模式取决于你的**使用场景**和**分析需求**，而不是评测结果本身。

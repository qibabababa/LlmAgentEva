# 三阶段评测系统 - 指标说明文档

本文档详细说明三阶段连续评测系统中各个阶段的可量化评测指标。

---

## 📊 整体汇总指标

### 1. 总体统计
- **总测试用例数 (Total Test Cases)**: 系统中配置的测试用例总数
- **完整流程通过数 (End-to-End Pass)**: 三个阶段全部通过的测试用例数量
- **完整流程通过率 (End-to-End Pass Rate)**: `完整流程通过数 / 总测试用例数 × 100%`

### 2. 各阶段汇总
每个阶段独立统计：
- **阶段通过数 (Stage Pass Count)**: 该阶段通过评测的测试用例数量
- **阶段通过率 (Stage Pass Rate)**: `阶段通过数 / 总测试用例数 × 100%`
- **平均得分 (Average Score)**: 该阶段所有测试用例得分的算术平均值

### 汇总输出示例
```
📊 三阶段评测汇总统计
======================================================================

总测试用例数: 6

各阶段通过情况:
  阶段1 (任务分解): 5/6 通过 (83.3%)
    - 平均综合得分: 75.50%
  阶段2 (任务规划): 4/6 通过 (66.7%)
    - 平均综合得分: 68.20%
  阶段3 (任务执行): 3/6 通过 (50.0%)

完整流程通过: 3/6 (50.0%)

详细结果:
  1. Bug修复任务
     分解: ✅ | 规划: ✅ | 执行: ✅
  2. 代码转换任务
     分解: ✅ | 规划: ✅ | 执行: ❌
  3. 代码重构任务
     分解: ✅ | 规划: ✅ | 执行: ❌
  4. 环境配置任务
     分解: ✅ | 规划: ❌ | 执行: ❌
  5. 文档生成任务
     分解: ✅ | 规划: ❌ | 执行: ❌
  6. 函数拆分任务
     分解: ❌ | 规划: ❌ | 执行: ❌
```

---

## 🔹 阶段1：任务分解 (Task Decomposition)

### 核心指标

#### 1. 召回率 (Recall)
**定义**: 模型输出中包含了多少比例的ground truth任务

**计算公式**:
```
Recall = (模型输出中匹配到的ground truth任务数) / (ground truth任务总数)
```

**判断标准**:
- 使用LLM语义相似度判断（阈值默认0.6）
- 如果LLM不可用，回退到规则匹配方法

**示例**:
```
Ground Truth: ["任务A", "任务B", "任务C", "任务D", "任务E"]  (5个)
模型输出: ["任务A", "任务B的变体", "任务F", "任务C"]        (4个)

匹配结果:
  - "任务A" ✅ 匹配到 ground truth "任务A"
  - "任务B的变体" ✅ 匹配到 ground truth "任务B" (语义相似度0.85)
  - "任务F" ❌ 未匹配
  - "任务C" ✅ 匹配到 ground truth "任务C"

匹配数: 3
Recall = 3/5 = 60.00%
```

#### 2. 精确率 (Precision)
**定义**: 模型输出的任务中有多少比例是有效的（匹配ground truth）

**计算公式**:
```
Precision = (模型输出中匹配到的ground truth任务数) / (模型输出任务总数)
```

**示例**（使用上面的例子）:
```
模型输出: 4个任务
匹配数: 3个
Precision = 3/4 = 75.00%
```

#### 3. F1分数 (F1 Score)
**定义**: 召回率和精确率的调和平均数，综合评估模型表现

**计算公式**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**示例**（使用上面的例子）:
```
Precision = 75.00%
Recall = 60.00%
F1 = 2 × (0.75 × 0.60) / (0.75 + 0.60) = 0.6667 = 66.67%
```

#### 4. 综合得分 (Overall Score)
**定义**: F1分数，用于判断是否通过评测

**通过标准**:
- F1分数 ≥ 阈值（默认60%）
- 召回率 ≥ min_recall（可在测试用例中配置，默认60%）
- 精确率 ≥ min_precision（可在测试用例中配置，默认50%）

**配置示例**:
```json
"decomposition": {
  "ground_truth": ["任务1", "任务2", "任务3"],
  "min_recall": 0.6,      // 最低召回率60%
  "min_precision": 0.5    // 最低精确率50%
}
```

### 输出示例
```
🔹 阶段1：任务分解
----------------------------------------------------------------------
用户问题: 修复bug_code_1.py中的bug并保存到fix_code_1.py

模型输出: 5 个子任务
Ground Truth: 5 个子任务

验证结果:
  召回率 (Recall): 80.00% (4/5 个ground truth任务被覆盖)
  精确率 (Precision): 80.00% (4/5 个模型输出是有效的)
  F1分数: 80.00%
  综合得分: 80.00%
  ✅ 通过 (F1≥60%, Recall≥60%, Precision≥50%)
```

---

## 🔹 阶段2：任务规划 (Task Planning)

### 核心指标

#### 1. 覆盖度 (Coverage)
**定义**: 模型生成的执行计划中包含了多少比例的ground truth任务

**计算公式**:
```
Coverage = (计划中包含的ground truth任务数) / (ground truth任务总数)
```

**判断方法**: 使用批量语义相似度判断（优化后1次API调用）

**示例**:
```
Ground Truth任务: ["任务A", "任务B", "任务C", "任务D"]  (4个)
模型计划: [["任务A"], ["任务B", "任务X"], ["任务C"]]

计划中的任务: "任务A", "任务B", "任务X", "任务C"
匹配到的ground truth: "任务A", "任务B", "任务C"  (3个)

Coverage = 3/4 = 75.00%
```

#### 2. 顺序正确率 (Order Correctness)
**定义**: 有依赖关系的任务对中，有多少比例满足正确的执行顺序

**计算公式**:
```
Order Correctness = (顺序正确的依赖对数) / (总依赖对数)
```

**依赖检查规则**:
- 如果任务B依赖任务A，则A必须在B之前执行
- 在计划中，A所在的层级必须小于B所在的层级

**示例**:
```
依赖关系 (dependencies):
  - "任务B" 依赖 "任务A"
  - "任务C" 依赖 "任务A"
  - "任务D" 依赖 "任务B"

模型计划:
  层级1: ["任务A"]
  层级2: ["任务B", "任务C"]
  层级3: ["任务D"]

检查结果:
  ✅ "任务B"依赖"任务A": 任务A在层级1，任务B在层级2 (1 < 2) → 正确
  ✅ "任务C"依赖"任务A": 任务A在层级1，任务C在层级2 (1 < 2) → 正确
  ✅ "任务D"依赖"任务B": 任务B在层级2，任务D在层级3 (2 < 3) → 正确

Order Correctness = 3/3 = 100.00%
```

#### 3. 层级效率 (Level Efficiency)
**定义**: 评估计划的层级划分是否合理，避免过度串行化

**计算公式**:
```
Level Efficiency = min(1.0, ideal_levels / actual_levels)

其中:
  ideal_levels = 拓扑排序得到的最优层级数（基于依赖关系）
  actual_levels = 模型输出的实际层级数
```

**评分规则**:
- 如果 actual_levels ≤ ideal_levels: 效率 = 100%（完美或更优）
- 如果 actual_levels > ideal_levels: 效率 = ideal_levels / actual_levels

**示例**:
```
依赖关系:
  A → B → D
  A → C → D

理想层级数 (ideal_levels) = 3:
  层级1: [A]
  层级2: [B, C]  (可并行)
  层级3: [D]

情况1 - 模型输出3层:
  层级1: [A]
  层级2: [B, C]
  层级3: [D]
  Level Efficiency = min(1.0, 3/3) = 100.00%

情况2 - 模型输出4层（过度串行）:
  层级1: [A]
  层级2: [B]
  层级3: [C]
  层级4: [D]
  Level Efficiency = min(1.0, 3/4) = 75.00%
```

#### 4. 综合得分 (Overall Score)
**定义**: 三个指标的加权平均

**计算公式**:
```
Overall Score = (Coverage × 0.5) + (Order Correctness × 0.3) + (Level Efficiency × 0.2)
```

**权重说明**:
- Coverage: 50% - 最重要，确保所有任务都被包含
- Order Correctness: 30% - 重要，确保依赖关系正确
- Level Efficiency: 20% - 次要，优化并行性

**通过标准**:
- Overall Score ≥ 阈值（默认60%）
- Coverage ≥ min_coverage（可配置，默认70%）
- Order Correctness ≥ min_order_correctness（可配置，默认80%）

**配置示例**:
```json
"planning": {
  "ground_truth_plan": [[...], [...]],
  "dependencies": {...},
  "min_coverage": 0.7,              // 最低覆盖度70%
  "min_order_correctness": 0.8      // 最低顺序正确率80%
}
```

### 输出示例
```
🔹 阶段2：任务规划
----------------------------------------------------------------------
输入: ground_truth任务列表 (5 个)

模型规划结果:
  层级1: ["任务A"]
  层级2: ["任务B", "任务C"]
  层级3: ["任务D"]
  层级4: ["任务E"]

验证结果:
  覆盖度 (Coverage): 100.00% (5/5 个任务都在计划中)
  顺序正确率 (Order Correctness): 100.00% (6/6 个依赖关系正确)
  层级效率 (Level Efficiency): 100.00% (4层 = 理想层数)
  综合得分: 100.00%
  ✅ 通过 (Overall≥60%, Coverage≥70%, Order≥80%)
```

---

## 🔹 阶段3：任务执行 (Task Execution)

### 核心指标

#### 1. 任务通过率 (Task Pass Rate)
**定义**: 任务执行是否通过最终验证

**计算方式**: 根据任务类型调用对应的验证器

**任务类型及验证方法**:

##### fix_bug（Bug修复）
- **验证器**: `lib/validators/bugcode.py`
- **验证方法**: 
  1. 运行修复后的代码对测试数据进行处理
  2. 将输出结果与预期结果（test_case）逐一比对
  3. 所有测试用例都通过才算成功
- **通过标准**: `predicted_result == expected_result` (对每个测试用例)

**示例**:
```python
test_case = [0, 4, 6, 12, 21]  # 5个测试用例的预期结果
模型输出结果 = [0, 4, 6, 12, 21]  # 模型执行后的实际结果

验证:
  Test case 1: 预测=0   GT=0   -> PASS
  Test case 2: 预测=4   GT=4   -> PASS
  Test case 3: 预测=6   GT=6   -> PASS
  Test case 4: 预测=12  GT=12  -> PASS
  Test case 5: 预测=21  GT=21  -> PASS

结果: ✅ 通过 (5/5)
```

##### convert（代码转换）
- **验证器**: `lib/validators/convert.py`
- **验证方法**: 
  1. 检查转换后的JavaScript文件是否存在
  2. 运行测试用例验证功能一致性
  3. 检查语法正确性
- **通过标准**: 文件存在 && 所有测试用例通过

##### refactor（代码重构）
- **验证器**: `lib/validators/refactor.py`
- **验证方法**: 
  1. 检查指定的函数名是否已重命名
  2. 验证重命名映射是否正确（names字段）
  3. 运行代码确保功能不变
- **通过标准**: 所有函数名正确重命名 && 代码可运行

**示例**:
```python
names = {
  "add_two_numbers": "addTwoNumbers",
  "multiply_two_numbers": "multiplyTwoNumbers"
}

验证:
  ✅ "add_two_numbers" 已重命名为 "addTwoNumbers"
  ✅ "multiply_two_numbers" 已重命名为 "multiplyTwoNumbers"
  ✅ 代码可以正常运行

结果: ✅ 通过
```

##### env（环境配置）
- **验证器**: `lib/validators/env.py`
- **验证方法**: 
  1. 检查虚拟环境是否创建
  2. 检查依赖是否安装
  3. 运行目标文件验证环境配置
- **通过标准**: 虚拟环境存在 && 依赖已安装 && 文件运行成功

##### sum（文档生成）
- **验证器**: `lib/validators/summary.py`
- **验证方法**: 
  1. 检查README.md文件是否生成
  2. 使用LLM Judge评估文档质量（内容完整性、准确性、可读性）
  3. 回退到规则方法（检查关键内容）
- **通过标准**: 文件存在 && LLM评分≥阈值（默认70%）

**LLM Judge评估维度**:
- 内容完整性（30%）: 是否包含项目简介、功能说明、使用方法
- 准确性（40%）: 描述是否与源代码一致
- 可读性（30%）: 格式规范、结构清晰

##### split（函数拆分）
- **验证器**: `lib/validators/split.py`
- **验证方法**: 
  1. 检查原函数是否被拆分为多个子函数
  2. 使用LLM Judge评估拆分质量（模块化、可读性、功能一致性）
  3. 运行测试验证功能不变
- **通过标准**: 函数已拆分 && LLM评分≥阈值（默认70%） && 功能一致

**LLM Judge评估维度**:
- 模块化（35%）: 每个子函数职责单一
- 可读性（35%）: 代码更易理解
- 功能一致性（30%）: 拆分后功能与原函数相同

#### 2. 执行轮次 (Execution Rounds)
**定义**: 模型完成任务所用的对话轮次

**统计方式**: 记录从开始执行到任务完成的轮数

**示例**:
```
轮次1: 调用 list_files 工具
轮次2: 调用 read_file 工具
轮次3: 调用 write_file 工具
轮次4: 调用 run_command 工具

总轮次: 4
```

#### 3. 工具调用次数 (Tool Call Count)
**定义**: 模型调用工具的总次数和类型分布

**统计维度**:
- 总调用次数
- 每种工具的调用次数
- 独特工具类型数

**示例**:
```
总工具调用: 8次
工具类型分布:
  - read_file: 3次
  - write_file: 2次
  - run_command: 2次
  - list_files: 1次
独特工具数: 4种
```

#### 4. 输出字符数 (Output Characters)
**定义**: 模型生成的总输出字符数

**用途**: 评估模型响应的详细程度

### 通过标准
```
通过 (Pass): 验证器返回True
失败 (Fail): 验证器返回False
```

### 输出示例
```
🔹 阶段3：任务执行
----------------------------------------------------------------------
输入: ground_truth执行计划 (4 层)

任务类型: fix_bug

开始执行任务...

执行结果:
  任务: fix_bug_1
  通过: ✓
  轮次: 6
  工具调用: 12次
  工具类型: read_file(3), write_file(2), run_command(4), list_files(1), bash(2)
  输出字符数: 2847
  
验证结果:
---------- 验证结果 ----------
Test case 1: 预测=0      GT=0      -> PASS
Test case 2: 预测=4      GT=4      -> PASS
Test case 3: 预测=6      GT=6      -> PASS
Test case 4: 预测=12     GT=12     -> PASS
Test case 5: 预测=21     GT=21     -> PASS

  ✅ 通过 (5/5 测试用例通过)
```

---

## 📈 指标阈值配置

所有阈值都可以在测试用例JSON文件中配置：

### 示例配置
```json
{
  "id": "test_001",
  "name": "Bug修复任务",
  
  "stages": {
    "decomposition": {
      "ground_truth": [...],
      "min_recall": 0.6,        // 最低召回率60%
      "min_precision": 0.5      // 最低精确率50%
    },
    
    "planning": {
      "ground_truth_plan": [...],
      "dependencies": {...},
      "min_coverage": 0.7,              // 最低覆盖度70%
      "min_order_correctness": 0.8      // 最低顺序正确率80%
    },
    
    "execution": {
      "max_rounds": 15          // 最大执行轮次
    }
  }
}
```

### 默认阈值
如果测试用例中未指定，使用以下默认值：

| 指标 | 默认阈值 | 说明 |
|-----|---------|------|
| 分解 - 召回率 | 60% | 至少覆盖60%的ground truth任务 |
| 分解 - 精确率 | 50% | 至少50%的模型输出是有效的 |
| 分解 - F1分数 | 60% | 综合得分阈值 |
| 规划 - 覆盖度 | 70% | 至少包含70%的任务 |
| 规划 - 顺序正确率 | 80% | 至少80%的依赖关系正确 |
| 规划 - 综合得分 | 60% | 加权平均得分阈值 |
| 执行 - 最大轮次 | 15 | 超过15轮视为超时 |

---

## 🔧 语义相似度判断

### LLM-based判断
**使用模型**: 独立的Judge Model（配置在 `config.yaml` 的 `judge_api` 部分）

**判断提示词模板**:
```
请判断以下两个任务在语义上是否相似：
任务1: {task1}
任务2: {task2}

如果两个任务的核心目标和要解决的问题基本一致，即使表述不同，也应判断为相似。
请回答 "是" 或 "否"。
```

**批量优化**:
- 使用 `calculate_similarity_llm_batch()` 一次性判断多个任务对
- 将N×M个判断请求合并为1次API调用
- 大幅减少API限流问题（99%减少）

**相似度阈值**: 默认0.6（可配置）

### 规则方法（回退）
当LLM不可用时，使用以下规则方法：

1. **完全匹配**: 字符串完全相同
2. **归一化匹配**: 
   - 转换为小写
   - 去除标点符号
   - 去除多余空格
   - 比较归一化后的字符串
3. **关键词匹配**: 提取关键词后比较重叠度

---

## 📋 完整评测流程示例

### 输入
```json
{
  "id": "test_001",
  "name": "Bug修复任务",
  "initial_question": "修复bug_code_1.py中的bug",
  "task_data": {
    "tag": "fix_bug",
    "number": 1,
    "test_case": [0, 4, 6, 12, 21]
  },
  "stages": {
    "decomposition": {
      "ground_truth": [
        "列出当前目录下的文件",
        "读取 bug_code_1.py 文件内容",
        "分析代码找出bug",
        "修复bug并保存到 fix_code_1.py",
        "运行测试验证修复效果"
      ]
    },
    "planning": {
      "ground_truth_plan": [
        ["列出当前目录下的文件", "读取 bug_code_1.py 文件内容"],
        ["分析代码找出bug"],
        ["修复bug并保存到 fix_code_1.py"],
        ["运行测试验证修复效果"]
      ],
      "dependencies": {
        "读取 bug_code_1.py 文件内容": ["列出当前目录下的文件"],
        "分析代码找出bug": ["读取 bug_code_1.py 文件内容"],
        "修复bug并保存到 fix_code_1.py": ["分析代码找出bug"],
        "运行测试验证修复效果": ["修复bug并保存到 fix_code_1.py"]
      }
    }
  }
}
```

### 输出
```
======================================================================
[1/6] Bug修复任务
======================================================================

🔹 阶段1：任务分解
----------------------------------------------------------------------
模型输出: 5 个子任务
Ground Truth: 5 个子任务

指标:
  召回率 (Recall): 100.00% (5/5)
  精确率 (Precision): 100.00% (5/5)
  F1分数: 100.00%
  ✅ 通过

🔹 阶段2：任务规划
----------------------------------------------------------------------
输入: ground_truth任务列表 (5 个)
模型输出: 4 层计划

指标:
  覆盖度 (Coverage): 100.00% (5/5)
  顺序正确率 (Order Correctness): 100.00% (4/4)
  层级效率 (Level Efficiency): 100.00% (4/4)
  综合得分: 100.00%
  ✅ 通过

🔹 阶段3：任务执行
----------------------------------------------------------------------
输入: ground_truth执行计划 (4 层)

指标:
  执行轮次: 6
  工具调用: 12次
  输出字符数: 2847

验证结果:
  Test case 1: 预测=0   GT=0   -> PASS
  Test case 2: 预测=4   GT=4   -> PASS
  Test case 3: 预测=6   GT=6   -> PASS
  Test case 4: 预测=12  GT=12  -> PASS
  Test case 5: 预测=21  GT=21  -> PASS
  ✅ 通过 (5/5)

======================================================================
整体结果: 分解: ✅ | 规划: ✅ | 执行: ✅
======================================================================
```

---

## 📊 汇总统计公式

### 阶段通过率
```
阶段通过率 = (该阶段通过的测试用例数) / (总测试用例数) × 100%
```

### 阶段平均得分
```
阶段平均得分 = Σ(各测试用例在该阶段的得分) / (总测试用例数)
```

### 完整流程通过率
```
完整流程通过率 = (三个阶段全部通过的测试用例数) / (总测试用例数) × 100%
```

---

## 🎯 指标使用建议

### 1. 模型能力评估
- **任务分解能力**: 关注召回率（是否遗漏关键任务）和精确率（是否产生无关任务）
- **任务规划能力**: 关注顺序正确率（依赖关系理解）和层级效率（并行化能力）
- **任务执行能力**: 关注最终通过率和工具使用效率

### 2. 模型对比
- 使用相同的测试用例集
- 对比各阶段的平均得分
- 重点关注完整流程通过率

### 3. 系统优化
- 召回率低 → 提示词需要更明确地要求完整性
- 精确率低 → 提示词需要更清晰地定义任务边界
- 顺序正确率低 → 增强依赖关系的提示
- 层级效率低 → 优化并行性提示

### 4. 测试用例设计
- Ground truth应涵盖任务的所有关键步骤
- 依赖关系应清晰且符合实际逻辑
- 验证数据应具有代表性

---

## 📚 相关文档

- [三阶段设计文档](THREE_STAGE_DESIGN.md) - 系统架构和设计理念
- [三阶段实现总结](THREE_STAGE_SUMMARY.md) - 实现细节和数据流
- [批量vs逐个处理](BATCH_VS_SEQUENTIAL.md) - 两种处理模式对比
- [快速开始](QUICK_START.md) - 系统使用指南
- [配置指南](CONFIGURATION.md) - 配置说明

---

**版本**: v2.1.0  
**最后更新**: 2024-12-10
